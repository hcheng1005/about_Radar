# Self-Attention 自注意力机制

<div align=center>
<img src="images/20231126151347.png" width="70%" >
</div>


## 链接
[台大李宏毅自注意力机制和Transformer详解](https://www.bilibili.com/video/BV1v3411r78R?p=1&vd_source=285a0f3fdc2fd10c78e15004be5bcf60)

[图解 Transformer -- 李宏毅](https://zhuanlan.zhihu.com/p/349569097)

[Multi-headed Self-attention（多头自注意力）机制介绍 ](https://zhuanlan.zhihu.com/p/365386753)

[为什么Transformer 需要进行 Multi-head Attention？](https://www.zhihu.com/question/341222779)

---

- [链接](#链接)
- [Q K V](#q-k-v)
  - [总结](#总结)
- [Multi-head self-Attention](#multi-head-self-attention)
- [Positional Encoding](#positional-encoding)

## Q K V

对于序列a1-a4，如何确定彼此之间的联系？

<div align=center>
<img src="images/20230415184745.png" width="70%" >
</div>


<div align=center>
<img src="images/20230415184840.png" width="70%" >
</div>


### 总结

<div align=center>
<img src="images/20231126145300.png" width="70%" >
</div>


## Multi-head self-Attention

<div align=center>
<img src="images/20231126150542.png" width="70%" >
</div>


<div align=center>
<img src="images/20231126150557.png" width="70%" >
</div>


<div align=center>
<img src="images/20231126150618.png" width="70%" >
</div>


## Positional Encoding

两种方式：**人为设定与自动学习**

<div align=center>
<img src="images/20231126150842.png" width="70%" >
</div>
