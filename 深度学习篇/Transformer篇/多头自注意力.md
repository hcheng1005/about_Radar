# 多头自注意力

<div align=center>
<img src="images/20240327211538.png" width="80%" >
</div>

---

[Multi-headed Self-attention（多头自注意力）机制介绍👍👍](https://zhuanlan.zhihu.com/p/365386753)

<div align=center>
<img src="images/20231126151347.png" width="80%" >
</div>

<div align=center>
<img src="images/20240327204707.png" width="80%" >
</div>

<div align=center>
<img src="images/20240327204714.png" width="80%" >
</div>

<div align=center>
<img src="images/20240327204731.png" width="80%" >
</div>

---

<div align=center>
<img src="images/20240327202823.png" width="80%" >
</div>

<div align=center>
<img src="images/20240327205927.png" width="80%" >
</div>


## 参考资料

[Multi-headed Self-attention（多头自注意力）机制介绍👍👍](https://zhuanlan.zhihu.com/p/365386753)

[为什么Transformer需要进行Multi-head Attention？👍👍👍👍](https://www.zhihu.com/question/341222779)
