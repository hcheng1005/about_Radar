# 神经网络

## 什么是神经网络


神经网络是深度学习的核心，模拟人脑的神经元网络来处理和学习复杂的数据模式。神经网络的基本框架包括以下几个关键组成部分：

1. **输入层（Input Layer**）
   
   - 作用: 接收输入数据。每个输入单元对应数据集中的一个特征。
  
2. **隐藏层（Hidden Layers**）
   
   - 作用: 在输入层和输出层之间，可以有一个或多个隐藏层。隐藏层负责进行数据的非线性变换，提取特征。
   - 组成: 每个隐藏层由多个神经元（或节点）组成，每个神经元都对前一层的输出进行加权求和，然后通过一个激活函数来引入非线性。

3. **输出层（Output Layer）**
    - 作用: 产生最终的输出。对于分类任务，输出层的神经元数量通常等于类别的数量；对于回归任务，输出层通常只有一个神经元。
  
4. **权重（Weights）和偏置（Biases）**
   - 作用: 神经网络的学习参数。每个连接线上都有一个权重，每个神经元都有一个偏置。通过学习算法调整这些参数，神经网络能够学习数据的特征和模式。
  
5. **激活函数（Activation Functions）**
   - 作用: 引入非线性，使得神经网络可以学习复杂的数据模式。常见的激活函数包括ReLU（Rectified Linear Unit）、Sigmoid和Tanh等。
  
6. **损失函数（Loss Function）**
   - 作用: 评估模型的预测值与实际值之间的差异。常见的损失函数有均方误差（MSE）用于回归任务，交叉熵损失（Cross-Entropy Loss）用于分类任务。
  
7. **优化算法（Optimization Algorithms）**
   - 作用: 调整神经网络的权重和偏置，以最小化损失函数。常用的优化算法包括梯度下降（Gradient Descent）、Adam等。
  
**工作流程：**
1. 前向传播（Forward Propagation）: 从输入层到输出层，逐层计算每个神经元的输出。

2. 计算损失（Compute Loss）: 使用损失函数计算模型输出与实际标签之间的差异。

3. 反向传播（Backward Propagation）: 从输出层到输入层，逐层计算损失函数相对于每个参数的梯度，并更新参数。
   
通过这个过程，神经网络能够学习数据中的复杂模式和关系，从而进行有效的预测和分类。


**简单的pytorch示例**
```python
# nn是神经网络的缩写
from torch import nn

"""构造一个线性模型"""
net = nn.Sequential(nn.Linear(2, 1))

"""初始化模型的权重和偏置"""
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

"""定义损失函数，这里是L2损失"""
loss = nn.MSELoss()

"""定义优化器"""
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

"""开始模型训练"""
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y) # 模型前向传播并计算loss
        trainer.zero_grad() # 清空之前的梯度
        l.backward() # 自动计算梯度
        trainer.step() # 模型参数更新
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```


## 激活函数
激活函数（activation functions）的目标是，将神经网络非线性化。激活函数是**连续的（continuous）**，且**可导的（differential）**。

> 连续的：当输入值发生较小的改变时，输出值也发生较小的改变；
> 
> 可导的：在定义域中，每一处都是存在导数

下面介绍几种常见的激活函数;

### ReLU