# PointPillars: 模型部署与代码详解

## 模型部署
[该仓库](https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars)的模型转onnx的过程需要特定的openpcdet版本. 因此修改了一版本的[export脚本](https://github.com/hcheng1005/OpenPCDet/blob/master/tools/onnx_utils_dual_radar/trans_pointpillar.py).

[export.py](./code/trans_pointpillar.py)

### 1.基础模型导出

#### a. 模型重写

模型重写主要是为了清晰模型各个子模块的流程,删除不必要的判断分支.

另外,需要调整部分模型的输入输出.

#### b. 权重读取

```python
"""基于重写的模型定义一个PP"""
model = pointpillars(cfg, np.array([gridx ,gridy, 1]))  
model.to('cuda').eval()

"""加载pth文件"""
checkpoint = torch.load(ckpt, map_location='cuda')

"""获取子模块权重参数"""
dicts = {}
for key in checkpoint["model_state"].keys():
    if "vfe" in key:
        dicts[key] = checkpoint["model_state"][key]
    if "backbone_2d" in key:
        dicts[key] = checkpoint["model_state"][key]
    if "dense_head" in key:
        dicts[key] = checkpoint["model_state"][key]
model.load_state_dict(dicts)
```

#### c. 输入定义

```python
with torch.no_grad():
    MAX_VOXELS = 10000
    dummy_voxels = torch.zeros(
        (MAX_VOXELS, 32, 4),
        dtype=torch.float32,
        device='cuda:0')

    dummy_voxel_idxs = torch.zeros(
        (MAX_VOXELS, 4),
        dtype=torch.int32,
        device='cuda:0')

    dummy_voxel_num = torch.zeros(
        (1),
        dtype=torch.int32,
        device='cuda:0')

    # pytorch don't support dict when export model to onnx.
    # so here is something to change in networek input and output, the dict input --> list input
    # here is three part onnx export from OpenPCDet codebase:
    dummy_input = (dummy_voxels, dummy_voxel_num, dummy_voxel_idxs)
```
#### d. 导出

```python
# 导出pp模型
torch.onnx.export(model,
                dummy_input,
                export_onnx_file,
                export_params=True,        # store the trained parameter weights inside the model file
                opset_version=11,          # the ONNX version to export the model to
                do_constant_folding=True,  # whether to execute constant folding for optimization
                keep_initializers_as_inputs=True,
                input_names = ['voxels', 'voxel_num', 'voxel_idxs'],   # the model's input names
                output_names = ['cls_preds', 'box_preds', 'dir_cls_preds'])# the model's output names
```

### 2. 模型简化
[源文件](https://github.com/hcheng1005/OpenPCDet/blob/master/tools/onnx_utils_dual_radar/simplifier_onnx.py)

[本地文件](./code/simplifier_onnx.py)

####  a. PPScatterPlugin自定义因子

```python
@gs.Graph.register()
def replace_with_clip(self, inputs, outputs):
    for inp in inputs:
        inp.outputs.clear()

    for out in outputs:
        out.inputs.clear()

    op_attrs = dict()
    # op_attrs["dense_shape"] = np.array([496, 640]) #
    op_attrs["dense_shape"] = np.array([248, 320]) # 距离/分辨率

    return self.layer(name="PPScatter_0", op="PPScatterPlugin", inputs=inputs, outputs=outputs, attrs=op_attrs)
```

在[原始工程仓库](https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars)生成trt的脚本如下:

```bash
#!/bin/bash
/usr/src/tensorrt/bin/trtexec --onnx=./model/pointpillar.onnx --fp16 --plugins=build/libpointpillar_core.so --saveEngine=./model/pointpillar.plan --inputIOFormats=fp16:chw,int32:chw,int32:chw --verbose --dumpLayerInfo --dumpProfile --separateProfileRun --profilingVerbosity=detailed > model/pointpillar.8611.log 2>&1
```

GPT解释:

```bash
您提供的命令是一个脚本，使用TensorRT可执行程序trtexec对名为pointpillar.onnx的ONNX模型执行各种操作。以下是命令及其选项的详细说明：

--onnx=./model/pointpillar.onnx：指定ONNX模型文件pointpillar.onnx的路径，该文件位于./model目录中。
--fp16：启用模型在降低精度（FP16）下执行。
--plugins=build/libpointpillar_core.so：指定包含模型所需的自定义TensorRT插件的插件库libpointpillar_core.so的路径。
--saveEngine=./model/pointpillar.plan：在TensorRT优化后，将优化的引擎保存到指定的文件pointpillar.plan中，该文件位于./model目录中。
--inputIOFormats=fp16:chw,int32:chw,int32:chw：指定模型输入数据的格式。输入数据格式设置为FP16和int32的CHW（通道、高度、宽度）格式。
--verbose：在模型执行期间启用详细输出。
--dumpLayerInfo：在模型执行期间转储层信息。
--dumpProfile：在模型执行期间转储性能分析信息。
--separateProfileRun：为每个层运行单独的性能分析。
--profilingVerbosity=detailed：将性能分析信息的详细程度设置为详细。
> model/pointpillar.8611.log 2>&1：将标准输出和标准错误重定向到日志文件pointpillar.8611.log，该文件位于./model目录中。
总体而言，此命令执行TensorRT引擎创建过程，用于pointpillar.onnx模型，具有特定配置，如FP16精度、自定义插件、输入数据格式和详细的性能分析信息。输出和错误消息被重定向到日志文件，以供进一步分析。
```

上述有个关键的地方是就是`plugins=build/libpointpillar_core.so`.

关于该so文件的生成如下:

```bash
sudo apt-get install git-lfs && git lfs install
git clone https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars.git
cd CUDA-PointPillars && . tool/environment.sh
mkdir build && cd build
cmake .. && make -j$(nproc)
cd ../ && sh tool/build_trt_engine.sh
cd build && ./pointpillar ../data/ ../data/ --timer
```

#### b. TBD

## 工程代码详解

### 
