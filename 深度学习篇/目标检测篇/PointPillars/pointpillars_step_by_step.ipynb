{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointPillars\n",
    "\n",
    "![](./images/20231115210105.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.特征编码 Feature Encoder\n",
    "按照点云数据所在的X，Y轴（不考虑Z轴）将点云数据划分为一个个的网格，凡是落入到一个网格的点云数据被视为其处在一个pillar里，或者理解为它们构成了一个Pillar。\n",
    "\n",
    "`每个点云用一个D=9维的向量表示，分别为$(x, y, z, r, x_c, y_c, z_c, x_p, y_p)$ `。\n",
    "\n",
    "其中$x, y, z, r$为该点云的真实坐标信息（三维）和反射强度； \n",
    "\n",
    "$x_c, y_c, z_c$为该点云所处Pillar中所有点的几何中心；\n",
    "\n",
    "$x_p, y_p$为表示该激光点云相对Pillar坐标的偏移量。\n",
    "\n",
    "假设每个样本中有$P$个非空的pillars，每个pillar中有$N$个点云数据，那么这个样本就可以用一个$(D, P, N)$张量表示。\n",
    "\n",
    "那么可能就有人问了，怎么保证每个pillar中有$N$个点云数据呢？\n",
    "\n",
    "如果每个pillar中的点云数据数据超过$N$个，那么我们就随机采样至$N$个；如果每个pillar中的点云数据数据少于$N$个，少于的部分我们就填充为0；\n",
    "\n",
    "---\n",
    "\n",
    "下面是openPCDet关于特征编码的实现（openpcdet/pcdet/models/backbones_3d/vfe/pillar_vfe.py）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_features, voxel_num_points, coords = batch_dict['voxels'], batch_dict['voxel_num_points'], batch_dict['voxel_coords']\n",
    "\n",
    "# 计算均值\n",
    "points_mean = voxel_features[:, :, :3].sum(dim=1, keepdim=True) / voxel_num_points.type_as(voxel_features).view(-1, 1, 1)\n",
    "\n",
    "# 计算【偏移】xc,yc,zc\n",
    "f_cluster = voxel_features[:, :, :3] - points_mean\n",
    "\n",
    "# 创建每个点云到该pillar的坐标中心点偏移量空数据 xp,yp,zp\n",
    "#  coords是每个网格点的坐标，即[432, 496, 1]，需要乘以每个pillar的长宽得到点云数据中实际的长宽（单位米）\n",
    "#  同时为了获得每个pillar的中心点坐标，还需要加上每个pillar长宽的一半得到中心点坐标\n",
    "#  每个点的x、y、z减去对应pillar的坐标中心点，得到每个点到该点中心点的偏移量\n",
    "f_center = torch.zeros_like(voxel_features[:, :, :3])\n",
    "\n",
    "# print(\"voxel_coords\", coords[:,0])\n",
    "f_center[:, :, 0] = voxel_features[:, :, 0] - (coords[:, 3].to(voxel_features.dtype).unsqueeze(1) * self.voxel_x + self.x_offset)\n",
    "f_center[:, :, 1] = voxel_features[:, :, 1] - (coords[:, 2].to(voxel_features.dtype).unsqueeze(1) * self.voxel_y + self.y_offset)\n",
    "f_center[:, :, 2] = voxel_features[:, :, 2] - (coords[:, 1].to(voxel_features.dtype).unsqueeze(1) * self.voxel_z + self.z_offset)\n",
    "\n",
    "# 如果使用绝对坐标，直接组合\n",
    "if self.use_absolute_xyz: #(True)\n",
    "    features = [voxel_features, f_cluster, f_center] # 4+3+3=10维\n",
    "else:\n",
    "    features = [voxel_features[..., 3:], f_cluster, f_center]\n",
    "    \n",
    "# 将特征在最后一维度拼接 得到维度为（M，32,10）的张量\n",
    "features = torch.cat(features, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Stacked Pillars --> Learned Features\n",
    "\n",
    "经过刚刚的处理，顺利得到`Stacked Pillars`。\n",
    "\n",
    "`Stacked Pillars`到`Learned Features`的转换过程非常简单，可以表述为P*N*D(30000 x 20 x 9)->P*N*C(30000 x 20 x 64)-> P*C(30000*64)。\n",
    "\n",
    "对应的处理流程可参考代码如下，**`先是经过一个Linear+BN+ReLU`**，然后 **`通过MaxPooling`** 操作将每个Pillar中最大响应的点云提取出来。\n",
    "\n",
    "上述中的Linear+BN+ReLU可以堆叠，论文使用了最简单的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义PFN类\n",
    "class PFNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 use_norm=True,\n",
    "                 last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.last_vfe = last_layer\n",
    "        self.use_norm = use_norm\n",
    "        if not self.last_vfe:\n",
    "            out_channels = out_channels // 2\n",
    "        \n",
    "        # x的维度由（M, 32, 10）升维成了（M, 32, 64）,max pool之后32才去掉\n",
    "        if self.use_norm: # True\n",
    "            self.linear = nn.Linear(in_channels, out_channels, bias=False)  # 线性层\n",
    "            self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01) # BN层\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_channels, out_channels, bias=True)\n",
    "\n",
    "        self.part = 50000\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] > self.part:\n",
    "            num_parts = inputs.shape[0] // self.part\n",
    "            part_linear_out = [self.linear(inputs[num_part*self.part:(num_part+1)*self.part])\n",
    "                               for num_part in range(num_parts+1)]\n",
    "            x = torch.cat(part_linear_out, dim=0)\n",
    "        else:\n",
    "            x = self.linear(inputs)\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        #permute变换维度，(M, 64, 32) --> (M, 32, 64)\n",
    "          # 这里之所以变换维度，是因为BatchNorm1d在通道维度上进行,对于图像来说默认模式为[N,C,H*W],通道在第二个维度上\n",
    "        x = self.norm(x.permute(0, 2, 1)).permute(0, 2, 1) if self.use_norm else x\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        \n",
    "        x = F.relu(x) # ReLU\n",
    "        \n",
    "        # 完成pointnet的MAXPooling操作，找出每个pillar中最能代表该pillar的点\n",
    "        x_max = torch.max(x, dim=1, keepdim=True)[0]\n",
    "\n",
    "        if self.last_vfe:\n",
    "            return x_max\n",
    "        else:\n",
    "            x_repeat = x_max.repeat(1, inputs.shape[1], 1)\n",
    "            x_concatenated = torch.cat([x, x_repeat], dim=2)\n",
    "            return x_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多个PFN结构串联\n",
    "pfn_layers = []\n",
    "for i in range(len(num_filters) - 1):\n",
    "    in_filters = num_filters[i]\n",
    "    out_filters = num_filters[i + 1]\n",
    "    pfn_layers.append(\n",
    "        PFNLayer(in_filters, out_filters, self.use_norm, last_layer=(i >= len(num_filters) - 2))\n",
    "    )\n",
    "# 加入线性层，将13维特征变为64维特征\n",
    "self.pfn_layers = nn.ModuleList(pfn_layers)\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "voxel_count = features.shape[1]\n",
    "\n",
    "# mask中指名了每个pillar中哪些是需要被保留的数据\n",
    "mask = self.get_paddings_indicator(voxel_num_points, voxel_count, axis=0)\n",
    "\n",
    "# （M， 32）->(M, 32, 1)\n",
    "mask = torch.unsqueeze(mask, -1).type_as(voxel_features)\n",
    "\n",
    "# 将feature中被填充数据的所有特征置0\n",
    "features *= mask\n",
    "for pfn in self.pfn_layers:\n",
    "    features = pfn(features)\n",
    "    \n",
    "# (M, 64), 每个pillar抽象出一个64维特征\n",
    "features = features.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.生成伪图像\n",
    "通过Scatter运算实现的。在openpcdet工程中，实际上在训练数据集制作阶段，就已经生成了坐标位置信息`batch_dict['voxel_coords']`，其维度是P*2,P是Pillar的数量，2是x和y对应的坐标。在Learned Features构造Pseudo Images的时候根据Pillar Index，将Pillar填充到对应的Pseudo Images上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointPillarScatter(nn.Module):\n",
    "    def __init__(self, model_cfg, grid_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_cfg = model_cfg\n",
    "        self.num_bev_features = self.model_cfg.NUM_BEV_FEATURES #64\n",
    "        self.nx, self.ny, self.nz = grid_size # [432,496,1]\n",
    "        assert self.nz == 1\n",
    "\n",
    "    def forward(self, batch_dict, **kwargs):\n",
    "        '''\n",
    "        batch_dict['pillar_features']-->为VFE得到的数据(M, 64)\n",
    "        voxel_coords:(M,4) --> (batch_index,z,y,x) batch_index代表了该点云数据在当前batch中的index\n",
    "        '''\n",
    "        pillar_features, coords = batch_dict['pillar_features'], batch_dict['voxel_coords']\n",
    "        batch_spatial_features = []\n",
    "        # 根据batch_index，获取batch_size大小\n",
    "        batch_size = coords[:, 0].max().int().item() + 1\n",
    "        for batch_idx in range(batch_size):\n",
    "            # 创建一个空间坐标所有用来接受pillar中的数据\n",
    "            # spatial_feature 维度 (64,214272)\n",
    "            spatial_feature = torch.zeros(\n",
    "                self.num_bev_features,\n",
    "                self.nz * self.nx * self.ny,\n",
    "                dtype=pillar_features.dtype,\n",
    "                device=pillar_features.device)\n",
    "\n",
    "            batch_mask = coords[:, 0] == batch_idx #返回mask，[True, False...]\n",
    "            this_coords = coords[batch_mask, :] #获取当前的batch_idx的数\n",
    "            \n",
    "            #计算pillar的索引，该点之前所有行的点总和加上该点所在的列即可\n",
    "            indices = this_coords[:, 1] + this_coords[:, 2] * self.nx + this_coords[:, 3]\n",
    "            indices = indices.type(torch.long)  # 转换数据类型\n",
    "            pillars = pillar_features[batch_mask, :]\n",
    "            pillars = pillars.t()\n",
    "            \n",
    "            # 在索引位置填充pillar_features\n",
    "            spatial_feature[:, indices] = pillars\n",
    "            # 将空间特征加入list,每个元素为(64, 214272)\n",
    "            batch_spatial_features.append(spatial_feature)\n",
    "\n",
    "        # 在第0个维度将所有的数据堆叠在一起\n",
    "        batch_spatial_features = torch.stack(batch_spatial_features, 0)\n",
    "         # reshape回原空间(伪图像)    （4, 64, 214272）--> (4, 64, 496, 432)\n",
    "        batch_spatial_features = batch_spatial_features.view(batch_size, self.num_bev_features * self.nz, self.ny, self.nx)\n",
    "        batch_dict['spatial_features'] = batch_spatial_features\n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    " * @Author: CharlesHAO hao.cheng@wuzheng.com\n",
    " * @Date: 2024-03-22 14:38:14\n",
    " * @LastEditors: CharlesHAO hao.cheng@wuzheng.com\n",
    " * @LastEditTime: 2024-03-22 17:13:50\n",
    " * @FilePath: /about_Radar/深度学习篇/目标检测篇/PointPillars/pp_stepBystep.ipynb\n",
    " * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "-->\n",
    "## 4.BackBone网络\n",
    "伪图片作者2D CNN的输入，用来进一步提取图片特征。\n",
    "\n",
    "从图中可以看出，该2D CNN采用了两个网络。其中一个网络不断缩小特征图的分辨率，同时提升特征图的维度，因此获得了三个不同分辨率的特征图。\n",
    "\n",
    "另一个网络对 **`三个特征图进行上采样至相同大小，然后进行concatenation。`**\n",
    "\n",
    "之所以选择这样架构，是因为**不同分辨率的特征图负责不同大小物体的检测。比如分辨率大的特征图往往感受野较小，适合捕捉小物体（在KITTI中就是行人）**。\n",
    "\n",
    "BackBone模型结构体如下:\n",
    "\n",
    "第一部分：三个降采样模块\n",
    "```cmd\n",
    " (0): Sequential(\n",
    "        (0): ZeroPad2d((1, 1, 1, 1))\n",
    "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "        (2): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (3): ReLU()\n",
    "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (5): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (6): ReLU()\n",
    "        (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (8): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (9): ReLU()\n",
    "        (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (11): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (12): ReLU()\n",
    "      )\n",
    "      (1): Sequential(\n",
    "        (0): ZeroPad2d((1, 1, 1, 1))\n",
    "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "        (2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (3): ReLU()\n",
    "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (5): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (6): ReLU()\n",
    "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (8): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (9): ReLU()\n",
    "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (11): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (12): ReLU()\n",
    "        (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (14): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (15): ReLU()\n",
    "        (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (17): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (18): ReLU()\n",
    "      )\n",
    "      (2): Sequential(\n",
    "        (0): ZeroPad2d((1, 1, 1, 1))\n",
    "        (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "        (2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (3): ReLU()\n",
    "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (5): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (6): ReLU()\n",
    "        (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (8): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (9): ReLU()\n",
    "        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (11): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (12): ReLU()\n",
    "        (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (14): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (15): ReLU()\n",
    "        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (17): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (18): ReLU()\n",
    "      )\n",
    "```\n",
    "\n",
    "第二部分：三个上采样模块\n",
    "```\n",
    "    (deblocks): ModuleList(\n",
    "      (0): Sequential(\n",
    "        (0): ConvTranspose2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "      )\n",
    "      (1): Sequential(\n",
    "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "      )\n",
    "      (2): Sequential(\n",
    "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
    "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "      )\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "下面展示代码部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BaseBEVBackbone(nn.Module):\n",
    "    def __init__(self, model_cfg, input_channels):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg\n",
    "\n",
    "        if self.model_cfg.get('LAYER_NUMS', None) is not None:\n",
    "            assert len(self.model_cfg.LAYER_NUMS) == len(self.model_cfg.LAYER_STRIDES) == len(self.model_cfg.NUM_FILTERS)\n",
    "            layer_nums = self.model_cfg.LAYER_NUMS\n",
    "            layer_strides = self.model_cfg.LAYER_STRIDES\n",
    "            num_filters = self.model_cfg.NUM_FILTERS\n",
    "        else:\n",
    "            layer_nums = layer_strides = num_filters = []\n",
    "\n",
    "        if self.model_cfg.get('UPSAMPLE_STRIDES', None) is not None:\n",
    "            assert len(self.model_cfg.UPSAMPLE_STRIDES) == len(self.model_cfg.NUM_UPSAMPLE_FILTERS)\n",
    "            num_upsample_filters = self.model_cfg.NUM_UPSAMPLE_FILTERS\n",
    "            upsample_strides = self.model_cfg.UPSAMPLE_STRIDES\n",
    "        else:\n",
    "            upsample_strides = num_upsample_filters = []\n",
    "\n",
    "        num_levels = len(layer_nums)\n",
    "        c_in_list = [input_channels, *num_filters[:-1]]\n",
    "        \n",
    "        \"\"\"\"开始搭建模型结构\"\"\"\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.deblocks = nn.ModuleList()\n",
    "        \n",
    "        for idx in range(num_levels):\n",
    "            \"\"\"第一部分：pandding+Conv+BN+ReLU\"\"\"\n",
    "            cur_layers = [\n",
    "                nn.ZeroPad2d(1),\n",
    "                nn.Conv2d(\n",
    "                    c_in_list[idx], num_filters[idx], kernel_size=3,\n",
    "                    stride=layer_strides[idx], padding=0, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(num_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            \n",
    "            \"\"\"第二部分，经典结构: Conv+BN+ReLU\"\"\"\n",
    "            for k in range(layer_nums[idx]):\n",
    "                cur_layers.extend([\n",
    "                    nn.Conv2d(num_filters[idx], num_filters[idx], kernel_size=3, padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(num_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                    nn.ReLU()\n",
    "                ])\n",
    "                \n",
    "            self.blocks.append(nn.Sequential(*cur_layers))\n",
    "            \n",
    "            \"\"\"\"第三部分：deblock 上采样\"\"\"\n",
    "            if len(upsample_strides) > 0:\n",
    "                stride = upsample_strides[idx]\n",
    "                if stride > 1 or (stride == 1 and not self.model_cfg.get('USE_CONV_FOR_NO_STRIDE', False)):\n",
    "                    self.deblocks.append(nn.Sequential(\n",
    "                        nn.ConvTranspose2d(\n",
    "                            num_filters[idx], num_upsample_filters[idx],\n",
    "                            upsample_strides[idx],\n",
    "                            stride=upsample_strides[idx], bias=False\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(num_upsample_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "                else:\n",
    "                    stride = np.round(1 / stride).astype(np.int32)\n",
    "                    self.deblocks.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            num_filters[idx], num_upsample_filters[idx],\n",
    "                            stride,\n",
    "                            stride=stride, bias=False\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(num_upsample_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                        nn.ReLU()\n",
    "                    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 检测头部分\n",
    "pointpillars是一种基于锚框Anchor的目标检测方法。\n",
    "\n",
    "基于锚框（Anchor）的目标检测算法是一种常见的目标检测方法，主要用于在图像中识别和定位多个目标。这种方法的核心思想是在图像中预定义一系列不同形状和大小的锚框，然后通过模型预测这些锚框与真实目标之间的偏移量以及目标的类别。以下是基于锚框的目标检测逻辑的简要概述：\n",
    "\n",
    "- **预定义锚框**：在图像的不同位置预定义一系列固定大小和形状的锚框。这些锚框被设计为覆盖图像中可能出现的各种目标大小和形状。\n",
    "\n",
    "- **特征提取**：使用深度学习模型（如卷积神经网络CNN）从输入图像中提取特征。这些特征将用于后续的目标检测和分类。\n",
    "\n",
    "- **锚框调整和分类**：对于每个锚框，模型会预测两部分内容：\n",
    "\n",
    "  - 目标检测：预测锚框与其最匹配的真实目标之间的偏移量（通常包括中心点的偏移和宽高的缩放）。这允许模型调整锚框的位置和大小，使其更好地匹配真实目标。\n",
    "\n",
    "  - 目标分类：预测锚框内包含的目标的类别（如果有的话）。\n",
    "  \n",
    "- **非极大值抑制（NMS）**：由于每个目标可能与多个锚框匹配，因此会产生多个重叠的检测框。非极大值抑制是一种后处理步骤，用于在重叠的检测框中选择一个最佳的框，去除其他冗余的框，从而减少重复检测。\n",
    "\n",
    "- **输出检测结果**：最终，模型输出调整后的锚框位置、大小以及目标的类别，完成目标的检测和分类。\n",
    "\n",
    "\n",
    "首先看下完整的检测头代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: CharlesHAO hcheng1005@gmail.com\n",
    "Date: 2024-03-22 20:02:44\n",
    "LastEditors: CharlesHAO hcheng1005@gmail.com\n",
    "LastEditTime: 2024-03-23 13:42:11\n",
    "FilePath: /about_Radar/深度学习篇/目标检测篇/PointPillars/pointpillars_step_by_step.ipynb\n",
    "Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "'''\n",
    "# 检测头前向传播代码\n",
    "def forward(self, data_dict):\n",
    "        spatial_features_2d = data_dict['spatial_features_2d'] # （4，384，248，216）\n",
    "\n",
    "        # 每个anchor的类别预测-->(4,18,248,216)\n",
    "        # 4: batch_size\n",
    "        # 18: 3*2*3, 其中3是类别个数，2是由于每个类别设置两种方向的锚框\n",
    "        cls_preds = self.conv_cls(spatial_features_2d) \n",
    "        \n",
    "        # 每个anchor的box预测-->(4,42,248,216)\n",
    "        # 4: batch_size\n",
    "        # 42: 3*2*7，其中3是类别个数，2是由于每个类别设置两种方向的锚框，7是box属性[x,y,z,dx,dy,dz,theta]\n",
    "        box_preds = self.conv_box(spatial_features_2d) \n",
    "\n",
    "        cls_preds = cls_preds.permute(0, 2, 3, 1).contiguous()  \n",
    "        box_preds = box_preds.permute(0, 2, 3, 1).contiguous()  # [N, H, W, C] -->(4,248,216,42)\n",
    "        \n",
    "        # 将预测结果存入前传结果字典\n",
    "        self.forward_ret_dict['cls_preds'] = cls_preds\n",
    "        self.forward_ret_dict['box_preds'] = box_preds\n",
    "        \n",
    "        # 如果存在方向卷积层，则继续处理方向\n",
    "        if self.conv_dir_cls is not None: \n",
    "            dir_cls_preds = self.conv_dir_cls(spatial_features_2d) # 每个anchor的方向预测-->(4,12,248,216)\n",
    "            dir_cls_preds = dir_cls_preds.permute(0, 2, 3, 1).contiguous()  # [N, H, W, C] -->(4,248,216,12)\n",
    "            self.forward_ret_dict['dir_cls_preds'] = dir_cls_preds\n",
    "        else:\n",
    "            dir_cls_preds = None\n",
    "\n",
    "        if self.training:\n",
    "            targets_dict = self.assign_targets(gt_boxes=data_dict['gt_boxes']) # （4，39，8）\n",
    "            self.forward_ret_dict.update(targets_dict)\n",
    "\n",
    "        # 如果不是训练模式，则直接进行box的预测或对于双阶段网络要生成proposal(此时batch不为1)\n",
    "        if not self.training or self.predict_boxes_when_training:\n",
    "            # 输入为最开始的类别和box以及方向的预测，输出为展开后的预测\n",
    "            batch_cls_preds, batch_box_preds = self.generate_predicted_boxes(\n",
    "                batch_size=data_dict['batch_size'],\n",
    "                cls_preds=cls_preds, box_preds=box_preds, dir_cls_preds=dir_cls_preds\n",
    "            ) \n",
    "            data_dict['batch_cls_preds'] = batch_cls_preds # (1, 321408, 3)\n",
    "            data_dict['batch_box_preds'] = batch_box_preds # (1, 321408, 7)\n",
    "            data_dict['cls_preds_normalized'] = False\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码是一个目标检测模型。这个过程包括生成类别预测、框（bounding box）预测以及方向预测，并根据是否处于训练模式，进行目标分配或生成最终的预测框。下面是对这个过程的详细解读：\n",
    "\n",
    "`**输入**`\n",
    "data_dict: 包含模型输入数据和配置的字典，至少包含spatial_features_2d（空间特征图）和gt_boxes（真实框，仅在训练时使用）。\n",
    "\n",
    "`**处理流程**`\n",
    "\n",
    "- `**类别和框预测：**`\n",
    "\n",
    "    通过卷积层conv_cls和conv_box处理spatial_features_2d来分别生成类别预测和框预测。\n",
    "    类别预测的形状为(batch_size, 18, height, width)，框预测的形状为(batch_size, 42, height, width)。\n",
    "    预测结果的形状调整为(batch_size, height, width, channels)以方便后续处理。\n",
    "\n",
    "- `**方向预测：**`\n",
    "\n",
    "    如果定义了conv_dir_cls卷积层，则生成每个锚框的方向预测，形状为(batch_size, 12, height, width)。\n",
    "    同样地，调整预测结果的形状以方便处理。\n",
    "    \n",
    "- `**训练模式下的目标分配：**`\n",
    "\n",
    "    如果处于训练模式，会根据真实框（gt_boxes）为每个锚框分配目标，包括类别标签和回归目标，并更新forward_ret_dict。\n",
    "\n",
    "- `**预测模式或训练时的预测生成：**`\n",
    "\n",
    "    如果不是训练模式，或者即使在训练模式下也需要生成预测框（predict_boxes_when_training为True），则调用generate_predicted_boxes函数。\n",
    "    该函数基于类别预测、框预测和方向预测生成最终的预测框，并将这些预测存储回data_dict中。\n",
    "\n",
    "`**输出**`\n",
    "\n",
    "data_dict: 更新后的输入字典，包含生成的预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor LOSS\n",
    "[README](./基于anchor的目标检测流程.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
