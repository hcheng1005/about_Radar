{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0：定义LSTM模型类、制作Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class ECGClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(ECGClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM 输出大小：(batch_size, seq_length, hidden_dim)\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        # 选择最后一个时间步的输出\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2：定义模型、损失函数、优化器以及训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 检查CUDA是否可用，如果可用则使用CUDA，否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 读取数据集\n",
    "data_train = np.loadtxt('./data/ECG5000_TEST.txt')\n",
    "\n",
    "# 获取data和label\n",
    "data, labels = data_train[:, 1:], data_train[:, 0]-1.0 # 这里-1.0是为了将标签从0开始\n",
    "\n",
    "# 转换成tensor张量\n",
    "data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# 使用 np.unique 获取所有唯一标签\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# 获取唯一标签的数量\n",
    "num_unique_labels = len(unique_labels)\n",
    "# print(num_unique_labels)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(data, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# model = ECGClassifier(input_dim=140, hidden_dim=64, num_layers=4, num_classes=num_unique_labels)\n",
    "model = ECGClassifier(input_dim=140, hidden_dim=4, num_layers=2, num_classes=num_unique_labels)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印模型结构、参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: lstm\n",
      "Structure: LSTM(140, 4, num_layers=2, batch_first=True)\n",
      "\n",
      "Layer: fc\n",
      "Structure: Linear(in_features=4, out_features=5, bias=True)\n",
      "\n",
      "weight_ih_l0: torch.Size([16, 140])\n",
      "weight_hh_l0: torch.Size([16, 4])\n",
      "bias_ih_l0: torch.Size([16])\n",
      "bias_hh_l0: torch.Size([16])\n",
      "weight_ih_l1: torch.Size([16, 4])\n",
      "weight_hh_l1: torch.Size([16, 4])\n",
      "bias_ih_l1: torch.Size([16])\n",
      "bias_hh_l1: torch.Size([16])\n",
      " \n",
      "--------------------------------\n",
      "\n",
      "Layer: lstm.weight_ih_l0 | Size: torch.Size([16, 140]) | Number of Parameters: 2240\n",
      "Layer: lstm.weight_hh_l0 | Size: torch.Size([16, 4]) | Number of Parameters: 64\n",
      "Layer: lstm.bias_ih_l0 | Size: torch.Size([16]) | Number of Parameters: 16\n",
      "Layer: lstm.bias_hh_l0 | Size: torch.Size([16]) | Number of Parameters: 16\n",
      "Layer: lstm.weight_ih_l1 | Size: torch.Size([16, 4]) | Number of Parameters: 64\n",
      "Layer: lstm.weight_hh_l1 | Size: torch.Size([16, 4]) | Number of Parameters: 64\n",
      "Layer: lstm.bias_ih_l1 | Size: torch.Size([16]) | Number of Parameters: 16\n",
      "Layer: lstm.bias_hh_l1 | Size: torch.Size([16]) | Number of Parameters: 16\n",
      "Layer: fc.weight | Size: torch.Size([5, 4]) | Number of Parameters: 20\n",
      "Layer: fc.bias | Size: torch.Size([5]) | Number of Parameters: 5\n",
      " \n",
      "--------------------------------\n",
      "\n",
      "模型总参数量: 10084 Bytes \n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(f\"Layer: {name}\")\n",
    "    print(f\"Structure: {layer}\")\n",
    "    if hasattr(layer, 'activation'):\n",
    "        print(f\"Activation Function: {layer.activation}\")\n",
    "    print()\n",
    "    \n",
    "lstm_params = list(model.lstm.named_parameters())\n",
    "for name, param in lstm_params:\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "    \n",
    "print(\" \\n--------------------------------\\n\")\n",
    "  \n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Number of Parameters: {num_params}\")\n",
    "\n",
    "print(\" \\n--------------------------------\\n\")\n",
    "\n",
    "print(f\"模型总参数量: {total_params * 4} Bytes \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3：开始模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/282], Loss: 1.5363\n",
      "Epoch [1/80], Step [200/282], Loss: 1.4769\n",
      "Epoch [2/80], Step [100/282], Loss: 1.3862\n",
      "Epoch [2/80], Step [200/282], Loss: 1.3239\n",
      "Epoch [3/80], Step [100/282], Loss: 1.1876\n",
      "Epoch [3/80], Step [200/282], Loss: 1.1449\n",
      "Epoch [4/80], Step [100/282], Loss: 1.0492\n",
      "Epoch [4/80], Step [200/282], Loss: 0.9409\n",
      "Epoch [5/80], Step [100/282], Loss: 1.0232\n",
      "Epoch [5/80], Step [200/282], Loss: 0.7217\n",
      "Epoch [6/80], Step [100/282], Loss: 0.7310\n",
      "Epoch [6/80], Step [200/282], Loss: 0.6037\n",
      "Epoch [7/80], Step [100/282], Loss: 0.5061\n",
      "Epoch [7/80], Step [200/282], Loss: 0.4006\n",
      "Epoch [8/80], Step [100/282], Loss: 0.5376\n",
      "Epoch [8/80], Step [200/282], Loss: 0.2943\n",
      "Epoch [9/80], Step [100/282], Loss: 0.6489\n",
      "Epoch [9/80], Step [200/282], Loss: 0.5622\n",
      "Epoch [10/80], Step [100/282], Loss: 0.3877\n",
      "Epoch [10/80], Step [200/282], Loss: 0.2406\n",
      "Epoch [11/80], Step [100/282], Loss: 0.6402\n",
      "Epoch [11/80], Step [200/282], Loss: 0.3263\n",
      "Epoch [12/80], Step [100/282], Loss: 0.4799\n",
      "Epoch [12/80], Step [200/282], Loss: 0.3455\n",
      "Epoch [13/80], Step [100/282], Loss: 0.2836\n",
      "Epoch [13/80], Step [200/282], Loss: 0.4981\n",
      "Epoch [14/80], Step [100/282], Loss: 0.2946\n",
      "Epoch [14/80], Step [200/282], Loss: 0.2991\n",
      "Epoch [15/80], Step [100/282], Loss: 0.2165\n",
      "Epoch [15/80], Step [200/282], Loss: 0.1579\n",
      "Epoch [16/80], Step [100/282], Loss: 0.2874\n",
      "Epoch [16/80], Step [200/282], Loss: 0.2541\n",
      "Epoch [17/80], Step [100/282], Loss: 0.3044\n",
      "Epoch [17/80], Step [200/282], Loss: 0.0801\n",
      "Epoch [18/80], Step [100/282], Loss: 0.2424\n",
      "Epoch [18/80], Step [200/282], Loss: 0.8145\n",
      "Epoch [19/80], Step [100/282], Loss: 0.4206\n",
      "Epoch [19/80], Step [200/282], Loss: 0.0738\n",
      "Epoch [20/80], Step [100/282], Loss: 0.2943\n",
      "Epoch [20/80], Step [200/282], Loss: 0.0593\n",
      "Epoch [21/80], Step [100/282], Loss: 0.0793\n",
      "Epoch [21/80], Step [200/282], Loss: 0.3152\n",
      "Epoch [22/80], Step [100/282], Loss: 0.2666\n",
      "Epoch [22/80], Step [200/282], Loss: 0.2317\n",
      "Epoch [23/80], Step [100/282], Loss: 0.2550\n",
      "Epoch [23/80], Step [200/282], Loss: 0.0546\n",
      "Epoch [24/80], Step [100/282], Loss: 0.2873\n",
      "Epoch [24/80], Step [200/282], Loss: 0.2627\n",
      "Epoch [25/80], Step [100/282], Loss: 0.5638\n",
      "Epoch [25/80], Step [200/282], Loss: 0.2330\n",
      "Epoch [26/80], Step [100/282], Loss: 0.0441\n",
      "Epoch [26/80], Step [200/282], Loss: 0.0522\n",
      "Epoch [27/80], Step [100/282], Loss: 0.4379\n",
      "Epoch [27/80], Step [200/282], Loss: 0.1887\n",
      "Epoch [28/80], Step [100/282], Loss: 0.3637\n",
      "Epoch [28/80], Step [200/282], Loss: 0.0463\n",
      "Epoch [29/80], Step [100/282], Loss: 0.0666\n",
      "Epoch [29/80], Step [200/282], Loss: 0.4800\n",
      "Epoch [30/80], Step [100/282], Loss: 0.0310\n",
      "Epoch [30/80], Step [200/282], Loss: 0.0444\n",
      "Epoch [31/80], Step [100/282], Loss: 0.0543\n",
      "Epoch [31/80], Step [200/282], Loss: 0.2718\n",
      "Epoch [32/80], Step [100/282], Loss: 0.0173\n",
      "Epoch [32/80], Step [200/282], Loss: 0.3614\n",
      "Epoch [33/80], Step [100/282], Loss: 0.2814\n",
      "Epoch [33/80], Step [200/282], Loss: 0.7581\n",
      "Epoch [34/80], Step [100/282], Loss: 0.2857\n",
      "Epoch [34/80], Step [200/282], Loss: 0.0363\n",
      "Epoch [35/80], Step [100/282], Loss: 0.4685\n",
      "Epoch [35/80], Step [200/282], Loss: 0.0720\n",
      "Epoch [36/80], Step [100/282], Loss: 0.2523\n",
      "Epoch [36/80], Step [200/282], Loss: 0.0537\n",
      "Epoch [37/80], Step [100/282], Loss: 0.0605\n",
      "Epoch [37/80], Step [200/282], Loss: 0.0343\n",
      "Epoch [38/80], Step [100/282], Loss: 0.0454\n",
      "Epoch [38/80], Step [200/282], Loss: 0.3336\n",
      "Epoch [39/80], Step [100/282], Loss: 0.3540\n",
      "Epoch [39/80], Step [200/282], Loss: 0.2005\n",
      "Epoch [40/80], Step [100/282], Loss: 0.1953\n",
      "Epoch [40/80], Step [200/282], Loss: 0.3125\n",
      "Epoch [41/80], Step [100/282], Loss: 0.0750\n",
      "Epoch [41/80], Step [200/282], Loss: 0.1671\n",
      "Epoch [42/80], Step [100/282], Loss: 0.5457\n",
      "Epoch [42/80], Step [200/282], Loss: 0.1578\n",
      "Epoch [43/80], Step [100/282], Loss: 0.4348\n",
      "Epoch [43/80], Step [200/282], Loss: 0.0488\n",
      "Epoch [44/80], Step [100/282], Loss: 0.0432\n",
      "Epoch [44/80], Step [200/282], Loss: 0.2253\n",
      "Epoch [45/80], Step [100/282], Loss: 0.2159\n",
      "Epoch [45/80], Step [200/282], Loss: 0.0684\n",
      "Epoch [46/80], Step [100/282], Loss: 0.0304\n",
      "Epoch [46/80], Step [200/282], Loss: 0.6788\n",
      "Epoch [47/80], Step [100/282], Loss: 0.0251\n",
      "Epoch [47/80], Step [200/282], Loss: 0.4743\n",
      "Epoch [48/80], Step [100/282], Loss: 0.3826\n",
      "Epoch [48/80], Step [200/282], Loss: 0.1718\n",
      "Epoch [49/80], Step [100/282], Loss: 0.0591\n",
      "Epoch [49/80], Step [200/282], Loss: 0.2224\n",
      "Epoch [50/80], Step [100/282], Loss: 0.1737\n",
      "Epoch [50/80], Step [200/282], Loss: 0.1633\n",
      "Epoch [51/80], Step [100/282], Loss: 0.4331\n",
      "Epoch [51/80], Step [200/282], Loss: 0.1916\n",
      "Epoch [52/80], Step [100/282], Loss: 0.1970\n",
      "Epoch [52/80], Step [200/282], Loss: 0.3340\n",
      "Epoch [53/80], Step [100/282], Loss: 0.1792\n",
      "Epoch [53/80], Step [200/282], Loss: 0.5410\n",
      "Epoch [54/80], Step [100/282], Loss: 0.0407\n",
      "Epoch [54/80], Step [200/282], Loss: 0.1199\n",
      "Epoch [55/80], Step [100/282], Loss: 0.3622\n",
      "Epoch [55/80], Step [200/282], Loss: 0.0701\n",
      "Epoch [56/80], Step [100/282], Loss: 0.3244\n",
      "Epoch [56/80], Step [200/282], Loss: 0.2315\n",
      "Epoch [57/80], Step [100/282], Loss: 0.0623\n",
      "Epoch [57/80], Step [200/282], Loss: 0.1669\n",
      "Epoch [58/80], Step [100/282], Loss: 0.2611\n",
      "Epoch [58/80], Step [200/282], Loss: 0.0647\n",
      "Epoch [59/80], Step [100/282], Loss: 0.2238\n",
      "Epoch [59/80], Step [200/282], Loss: 0.2764\n",
      "Epoch [60/80], Step [100/282], Loss: 0.0567\n",
      "Epoch [60/80], Step [200/282], Loss: 0.2989\n",
      "Epoch [61/80], Step [100/282], Loss: 0.1524\n",
      "Epoch [61/80], Step [200/282], Loss: 0.1274\n",
      "Epoch [62/80], Step [100/282], Loss: 0.3435\n",
      "Epoch [62/80], Step [200/282], Loss: 0.3526\n",
      "Epoch [63/80], Step [100/282], Loss: 0.3480\n",
      "Epoch [63/80], Step [200/282], Loss: 0.2619\n",
      "Epoch [64/80], Step [100/282], Loss: 0.3047\n",
      "Epoch [64/80], Step [200/282], Loss: 0.3499\n",
      "Epoch [65/80], Step [100/282], Loss: 0.2960\n",
      "Epoch [65/80], Step [200/282], Loss: 0.0295\n",
      "Epoch [66/80], Step [100/282], Loss: 0.3485\n",
      "Epoch [66/80], Step [200/282], Loss: 0.0537\n",
      "Epoch [67/80], Step [100/282], Loss: 0.0529\n",
      "Epoch [67/80], Step [200/282], Loss: 0.3166\n",
      "Epoch [68/80], Step [100/282], Loss: 0.0966\n",
      "Epoch [68/80], Step [200/282], Loss: 0.0457\n",
      "Epoch [69/80], Step [100/282], Loss: 0.2705\n",
      "Epoch [69/80], Step [200/282], Loss: 0.2614\n",
      "Epoch [70/80], Step [100/282], Loss: 0.2289\n",
      "Epoch [70/80], Step [200/282], Loss: 0.0568\n",
      "Epoch [71/80], Step [100/282], Loss: 0.0199\n",
      "Epoch [71/80], Step [200/282], Loss: 0.3253\n",
      "Epoch [72/80], Step [100/282], Loss: 0.0347\n",
      "Epoch [72/80], Step [200/282], Loss: 0.1689\n",
      "Epoch [73/80], Step [100/282], Loss: 0.0992\n",
      "Epoch [73/80], Step [200/282], Loss: 0.1982\n",
      "Epoch [74/80], Step [100/282], Loss: 0.0393\n",
      "Epoch [74/80], Step [200/282], Loss: 0.3113\n",
      "Epoch [75/80], Step [100/282], Loss: 0.1424\n",
      "Epoch [75/80], Step [200/282], Loss: 0.0379\n",
      "Epoch [76/80], Step [100/282], Loss: 0.6399\n",
      "Epoch [76/80], Step [200/282], Loss: 0.2582\n",
      "Epoch [77/80], Step [100/282], Loss: 0.1208\n",
      "Epoch [77/80], Step [200/282], Loss: 0.2204\n",
      "Epoch [78/80], Step [100/282], Loss: 0.0993\n",
      "Epoch [78/80], Step [200/282], Loss: 0.1378\n",
      "Epoch [79/80], Step [100/282], Loss: 0.0905\n",
      "Epoch [79/80], Step [200/282], Loss: 0.2035\n",
      "Epoch [80/80], Step [100/282], Loss: 0.2676\n",
      "Epoch [80/80], Step [200/282], Loss: 0.2233\n"
     ]
    }
   ],
   "source": [
    "# 定义训练过程\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device='cpu'):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (signals, labels) in enumerate(train_loader):\n",
    "            # 前向传播\n",
    "            # print(signals.shape)\n",
    "            signals, labels = signals.to(device), labels.to(device)\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "                \n",
    "# 开始执行模型训练\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=80, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4：测试集验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test data: 94.08888888888889%\n",
      "Average loss: 0.1777266554449064\n"
     ]
    }
   ],
   "source": [
    "## Step4：测试集验证\n",
    "# 读取测试数据集\n",
    "data_test = np.loadtxt('./data/ECG5000_TEST.txt')\n",
    "\n",
    "# 获取data和label\n",
    "test_data, test_labels = data_train[:, 1:], data_train[:, 0]-1.0\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32).unsqueeze(1)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():  # 关闭梯度计算\n",
    "    for data, labels in test_loader:\n",
    "        if device.type == 'cuda':\n",
    "            data, labels = data.to(device), labels.to(device)  # Move data and labels to the device\n",
    "        outputs = model(data)\n",
    "        loss = F.cross_entropy(outputs, labels)  # 计算损失\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        y_pred.extend(predicted.cpu().numpy())  # 收集预测结果\n",
    "        y_true.extend(labels.cpu().numpy())     # 收集真实标签\n",
    "\n",
    "average_loss = total_loss / len(test_loader)\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on the test data: {100 * accuracy}%')\n",
    "print(f'Average loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: 绘制confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/08ljmbzn5sq3s0j06xf02qbm0000gn/T/ipykernel_27908/3463582496.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + list(np.unique(y_pred)))\n",
      "/var/folders/3s/08ljmbzn5sq3s0j06xf02qbm0000gn/T/ipykernel_27908/3463582496.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + list(np.unique(y_pred)))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoUUlEQVR4nO3deXxU1fn48c+ThIAgKEggJIECsgaQBMJmkYq4oLSAWwW1oqAoxa22X6W2v69LS7Wt2ror7n61UrSlIIqAuACKQlhEhKJoQBIimyI7SSbP74+5gQiZyVwyyb0z87xfr/vKzJm7PHMJT865595zRFUxxphEk+R1AMYY4wVLfsaYhGTJzxiTkCz5GWMSkiU/Y0xCsuRnjElIcZ38RGSoiKwTkfUiMsnreMIRkWdFZKuIrPY6luqISAMRWSIin4jIZyJyl9cxGeOWxOt9fiKSDHwOnAUUAkuB0aq6xtPAQhCRQcAe4EVV7e51POGIiACNVHWPiNQDFgE3qepHHodmTMTiuebXF1ivql+pagkwFRjhcUwhqeoC4Fuv44iEBu1x3tZzlvj8K2riVjwnv0xgU6X3hU6ZiQIRSRaRlcBWYJ6qfuxxSMa4Es/JT6oos9pJlKhqQFVzgCygr4j4uqluzJHiOfkVAq0rvc8CNnsUS9xS1Z3Ae8BQbyMxxp14Tn5LgY4i0k5EUoFRwEyPY4oLIpImIic6r48DzgT+62lQxrgUt8lPVcuA64E5wFpgmqp+5m1UoYnIK8BioLOIFIrIOK9jCqMV8K6IrCL4R2aeqs7yOCZjXInbW12MMSacuK35GWNMOJb8jDEJyZKfMSYhWfIzxiSkuE9+IjLe6xjciKV4YylWiK14YynWWBX3yQ+ItV+iWIo3lmKF2Io3lmKNSYmQ/Iwx5ii+us+vYbLoifWiu899AWiYHN19AmRk94z+ToFt23eQ1vykWtl3tMVSrBBb8dZWrBu+3sT27Tuqeu49Yh0aJem+QGR5o/ggc1TVl48+pngdQGUn1oPxbX0VUkh3Lnrb6xCMcS1v4Jk13se+gHJthP9P71xX1rzGB6wlsZFpjDG+IRJcYp0lP2OMa/HQWWDJzxjjmtX8jDEJR4AkS37GmERkzV5jTEKKh2ZvPCRwY0wdkwiXsPsQaS0i74rIWmf+55uc8jtFpEhEVjrLeZW2+a0zD/c6ETmnUnlvEfnU+ewhZ3rVsKzmZ4xxRYhaza8M+LWqLheRxsAyEZnnfPY3Vb3vB8cVySY4HUU3IAN4W0Q6qWoAeJzgI4EfAW8SnFNmdriDW83PGONaNGp+qlqsqsud17sJTjcRbnrZEcBUVT2oqgXAeoIzB7YCmqjqYg0+svYiMLK672DJzxjjjkByhAvQXETyKy1VDtggIm2BXKBi/ufrRWSViDwrIk2dslBzcWc6r48sD8uSnzHGlYpmbyQLsF1V8yotU47an8jxwL+Am1V1F8Em7MlADlAM3F/p0EfSMOVhWfIzxrgWjWYvgIjUI5j4XlbVfwOo6hZVDahqOfAU0NdZPdRc3IXO6yPLw7LkZ4xxLUk0oiUcp0f2GWCtqj5QqbxVpdXOB1Y7r2cCo0Skvoi0AzoCS1S1GNgtIv2dfV4BzKjuO1hvrzHGtSjd5vdj4BfApyKy0im7HRgtIjkEm64bgGsBVPUzEZkGrCHYUzzR6ekFmAA8DxxHsJc3bE8vWPIzxrgUrcfbVHURVefRN8NsMxmYXEV5PtDdzfEt+RljXIuDBzxi75rf96XKC18HeLSgjMcKyvjou/JDn338XTmPfBUsn7c1WBv+cm85UzaU8XhBGVM2lFGw9/D6z39dxiNflfHEhuCyt8y7Ua3fmjufzjn96dCjD/fe96BncUQqVuLdVFjE4HNH0rXXqXTLG8iDjz7pdUjVioVzmySRLX5WqzU/ERkKPAgkA0+r6r013WeSwNktkmjVQDhYrkzZEODkhsKegLJuj3Jd22RSkuRQImuYLIzOSqZxirD1oPJSYYBbTj6c8y/ISCajgbf/SoFAgIm3TGLe66+SlZlBn9POZviwoWR37expXKHEUrwpycnc/6e76JXbk92799B74BDOOuN0X8YKsXFuI+3J9btaq/mJSDLwKHAukE3wImZ2TffbOEVo5SSr+klCWn1hV5mSv1MZ2ExIcf7cNEoJ/mzVQGjsvE5LhbJyKCv3z7wlAEvyl9OhfVvat2tLamoqoy4ayYxZ1V6v9UwsxduqVTq9coPzrTRufDxdO3eiaHOxx1GFFhPnNsJ7/Pw++EFtNnv7AutV9StVLQGmEnw8JWp2lirFB5SsBsKOEmXjfuXpjWU8/3UZRfuPTnBr9yjpDQ4nSIAZxQGe2FDG+9vL8Woyp6LNxbTOOnxDelZmBkXF/v0PGmvxVtiw8WtWfPIp/fr09jqUkGLl3EbrPj8v1Wazt6pHUfoduZLzuMt4gBNcRFNSrkwrCjC0RRL1k4VyhQMBGNcmmc0H4LXiADe2S6ZicIetB5W3t5VzedbhqdwuaJVMk3rB5vO0onJW7YKeJ9T9P1lVSTeCQSk8E2vxAuzZs4cLL72Kv//ljzRp0tjrcEKKhXMrHHp0LabVZs0vokdOVHVKxaMvkU4xGdBgsurRJImujYNfoUmK0LWxICJkHicIwWkrAXaVKv8sCjAyPZlmqYfDalLvcPO5RxOh6IA3Nb+szAw2FRYdel9YtJmM9HRPYolErMVbWlrKhZdexWWXXMQFI37qdThhxcq5tWZveKEeRakRVWXmN+U0rw8Dmh0Ov0tjoWBfMHntKFECGpyv90BA+UdRgCFpSbRpePhfo1yVfU6nSECVz/coLep786/Vp3cuX3xZQMGGjZSUlDD1tf8wfJgvpzoFYiteVWXchJvp2rkTt9w4wetwqhUr59aaveEtBTo6j6EUERyH69Ka7nTTfli1S2mRCk9sKANgSPMkck8QZhQrjxWUkSwwMj0JEWHJznK+LYEFO8pZsCN4m8svspKplwQvFQYIaLA62q6h0MuDJi9ASkoKj9x/D+eM+DmBQDljrxhNt+wunsQSiViK94PFH/N/r0yjR7dscvqfDsCf7vwd5w09y9vAQoiVc+v3Wl0kpDYv8jsjsP6d4K0uzzp3Z4eU0UA0ZiYtX+6/i9DGVCdv4JnkL19Zo9TVvpHoH7pFdo3q8qWBZaqaV5Pj1ZZazTSq+iZhHlUxxsSmeKj5xUY1yxjjGyLx0dtryc8Y41oc5D5LfsYY9/z+3G4kLPkZY1wRYnBElCpY8jPGuGYdHsaYhGQdHsaYhBMcydlfIyMdC0t+xhjX7JqfMSbxxMCgBZGw5GeMccV6e40xCctqfsaYhCNAiiU/Y0wispqfMSYh2TU/Y0zCEazmZ4xJUFbzi7KM7J7cuehtr8OIyJ7bT/E6BFeO/9Mqr0Mw8UJsVBdjTAKKl6krLfkZY1yLg9xnyc8Y405wYAOvo6g5S37GGNfiYVSXeOi0McbUsWhMWi4irUXkXRFZKyKfichNTnkzEZknIl84P5tW2ua3IrJeRNaJyDmVynuLyKfOZw+JVH8zjiU/Y4wrFR0ekSzVKAN+rapdgf7ARBHJBiYB81W1IzDfeY/z2SigGzAUeExEKiYQfhwYD3R0lqHVHdySnzHGHedWl0iWcFS1WFWXO693A2uBTGAE8IKz2gvASOf1CGCqqh5U1QJgPdBXRFoBTVR1saoq8GKlbUKya37GGFdcDmnVXETyK72foqpTjtqnSFsgF/gYaKmqxRBMkCLSwlktE/io0maFTlmp8/rI8rAs+RljXHPxeNt2Vc0Lvy85HvgXcLOq7gpzua6qDzRMeVjW7DXGuBaNZi+AiNQjmPheVtV/O8VbnKYszs+tTnkh0LrS5lnAZqc8q4ry8N+h+vCMMeaHotTbK8AzwFpVfaDSRzOBMc7rMcCMSuWjRKS+iLQj2LGxxGki7xaR/s4+r6i0TUjW7DXGuCJAUsR3OYdtff4Y+AXwqYisdMpuB+4FponIOOBr4GIAVf1MRKYBawj2FE9U1YCz3QTgeeA4YLazhGXJzxjjTpTGtFLVRYSuIA4Jsc1kYHIV5flAdzfHt+RnjHHNxvMzxiSkCB6g8D1LfsYYlyQukl/c9vaOve5GWvyoK93zTvM0jgnzdtJuyhb6vrTtUNmfPtpNp6e3cOrL2zj15W3MKTgAQGlAGT93J/1e2kbvF7dy39I9h7a568NddHlmC+mPfVPn36Eqb82dT+ec/nTo0Yd773vQ63DC+tvDT9AtbyDd805j9JjxHDhwwOuQwvL9ua24yzmSxcdqLTwReVZEtorI6to6RjhXXj6Kt/4z1YtD/8Bl2ccxfWSzo8on5jbiw8vS+PCyNM5p1wCA6V8coCSgfHx5GgtHp/Hcp/vYuKsMgHPbNeC9Uc3rNPZQAoEAE2+ZxOzpU1mz7ANeeXU6a9au8zqsKhVtLuahx58if+E8VucvJFAeYOqr070OK6RYOLcVvb2RLH5Wm7n5eSJ4uLi2DBp4Ks2aNa1+xVo2MLM+TRtE9ksgAntLlbJyZX+ZUi8ZGqcG/4n6tkolvVFyNXuoG0vyl9OhfVvat2tLamoqoy4ayYxZ1d5Z4JmysjL27z9AWVkZ+/btJ6NVutchhRQr51ZEIlr8rNaSn6ouAL6trf3Huimf7KP/S9uYMG8n3x0oB2BkhwY0qid0eHor2c9u5cZex9Osgf/aDkWbi2mddfjRyazMDIqKiz2MKLTMjFb85qZf0qZLDq1O7s4JTZpw9pmDvQ4rpJg4t5He4ezv3Of3Vnl8urpHQ1ZdmcaHlzUnvVESty/cBUD+llKSBb4Y14LVV6Xx8PI9FHxf5nG0RwsOnPFDfv0r/913O5kx6y0KPlvG5vWfsnffPl565VWvwwopVs6t1fyiQETGi0i+iORv277D63DqRItGySQnCUkiXNm9Icu2lALw6rr9nPmj+tRLFtIaJtM/I5UVzmd+kpWZwabCokPvC4s2k5Huz6bk2+++T7u2bUhLa069evW4YPgwPvx4qddhhRQr51YkssXPPE9+qjpFVfNUNS+t+Uleh1MnvtkbOPT69fUHyD4peMdRVuNk3t9Ugqqyt7Scpd+U0qmp/+5G6tM7ly++LKBgw0ZKSkqY+tp/GD7Ms8u7YbVpncVHS5exb98+VJX57y2ga+eOXocVUiycW0GQpKSIFj/z3/+sKBk9ZjzvLfyA7Tu+JavjKdz1+1sZN+byOo/jqtnfsbCwhB0Hyun8zBZu79eYRUUlrNpWigBtmiTz0JATABh/SkMmzPuevi9tR4HLs4+je1o9AH6/aBevrtvPvlKl8zNbGNOtIbf3b1zn3wcgJSWFR+6/h3NG/JxAoJyxV4ymW3YXT2KpTr8+vblo5M/o9eMhpCSnkNuzB+PHXuF1WCHFxLmNgVpdJKSqawxR2bHIK8DpQHNgC3CHqj4Tbpu8Xjmab5OW1wqbtNwA5A08k/zlK2uUunqelKxvDT0uonUz/rF3WXXj+Xml1mp+qjq6tvZtjPFWPNT84rbZa4ypRXGQ/Sz5GWNci4PcZ8nPGOOOCCT5vCc3Epb8jDGuWc3PGJOA4uNeF0t+xhjX4iD3WfIzxrgk/nze2C1LfsYYV6I0f5HnLPkZY1zz+3O7kbDkZ4xxzWp+xpjEY9f8jDEJK/ZznyU/Y4w7FeP5xTpLfsYYd4LTt3kdRY1Z8jPGuCSIWPIzxiQi6/CIvtoaWTraGv1xhdchxDUtD1S/kk9Ikj/mU65TlvyMMYnIbnUxxiQeEYiDa36x/w2MMXVOkpMiWqrdj8izIrJVRFZXKrtTRIpEZKWznFfps9+KyHoRWSci51Qq7y0inzqfPSQRVE0t+Rlj3JOkyJbqPQ9UNTHx31Q1x1neBBCRbGAU0M3Z5jERqbjg+jgwHujoLNVOdmzJzxjjjggS4VIdVV0AfBvhkUcAU1X1oKoWAOuBviLSCmiiqos12GP6IjCyup1Z8jPGuCcS2XLsrheRVU6zuKlTlglsqrROoVOW6bw+sjwsS37GGPciT37NRSS/0jI+gr0/DpwM5ADFwP0VR61iXQ1THpb19hpjXAk+2xvxvY3bVTXPzf5VdcuhY4k8Bcxy3hYCrSutmgVsdsqzqigPy2p+xhh3BCRJIlqOaffBa3gVzgcqeoJnAqNEpL6ItCPYsbFEVYuB3SLS3+nlvQKYUd1xQtb8RORhwlQdVfXG6r+GMSYuRek+PxF5BTidYPO4ELgDOF1Ecgjmnw3AtQCq+pmITAPWAGXARFWteBRoAsGe4+OA2c4SVrhmb777r2KMSQhResJDVUdXUfxMmPUnA5OrKM8Hurs5dsjkp6ovuNmRMSZRRHYbi99V2+EhImnAbUA20KCiXFXPqMW4jDF+FSfTt0XScH8ZWAu0A+4i2AZfWosxGWN8TpKSI1r8LJLkd5KqPgOUqur7qjoW6F/LcRljfEsgKcLFxyK5z6/U+VksIsMI3j+TFWZ9Y0w8ExJmJOc/isgJwK+Bh4EmwK9qNSpjjL8lwjU/VZ2lqt+r6mpVHayqvVV1Zl0E59bYCTfRsm02PfoMOlR25+S/kNXxFHIHDCZ3wGDenPO2hxEeduDAAfqdPpScAWfQvc8g7pj8l0OfPfzE03TJ/THd+wzi1t/f7WGUob01dz6dc/rToUcf7r3vQa/D+YFQ5/bOP/2VrE455J46hNxTh/jmd+FIfj63h9T+s721LpLe3ueo4mZn59pfuO0aAAuA+s5xXlPVO44xzohcedkorr92HGOuuf4H5Tdffy2/uWlibR7atfr16zN/1r84/vhGlJaWctrZwzn3rCHsP7CfmW/M4ZOP3qF+/fps3bbN61CPEggEmHjLJOa9/ipZmRn0Oe1shg8bSnbXzl6HBoQ+twA3TxzPb276pccRhub3cwvO420+T2yRiKThPgt4w1nmE2z27olgu4PAGarak+ADykNFpFY7SgYNHECzpifW5iGiRkQ4/vhGAJSWllJaWoaI8MTTL3DbLTdQv359AFqkpXkZZpWW5C+nQ/u2tG/XltTUVEZdNJIZs6q9ob7OhDq3scDv5xZwpq5MjmzxsUiavf+qtLwM/JwI7qTWoIokWc9ZPJmd6NEnn6Vnv58wdsJNfPfdTi9CqFIgECD31CG0bN+dMwcPol+fXny+/isWfvgR/Qefy+lDR7J0mf8mSiraXEzrrMMjBmVlZlBUXOxhREer6twCPDrlWXr2H8zYCTf76nehQiycWyBq4/l56Vi6bDoCbSJZUUSSRWQlsBWYp6ofV7HO+IrhbrZt33EM4YQ34eorWf/pElYsfpdWLVvy69trteXtSnJyMis+nM+m/65g6bIVrF6zlrKyMr7b+T2L33mTv/zxf7lkzHjfzWhXVTx++0Wv6txOuPpK1q/6mBUfzqdVekt+ffudXod5lFg4t8FbXZIiW3ys2uhEZLeI7KpYgNcJPvFRLVUNqGoOwVtj+orIUTVGVZ2iqnmqmpfW/CSX4VevZcsWJCcnk5SUxDVXXc7SfP/VpE488QR+ctqpvDXvXbIyM7hg+HmICH3zepGUlMT2WvijUBNZmRlsKiw69L6waDMZ6ekeRhRa5XPbskXa4d+FKy/zZa06Zs5tHHR4RNLsbayqTSotnVT1X24Ooqo7gfeIYFz9aCv+5tDQYEx//U26Z3ep6xCqtG3bdnbu/B6A/fv3M//dhXTp1IERPx3KO+8vAuDzL76kpKSU5rXwR6Em+vTO5YsvCyjYsJGSkhKmvvYfhg+r83/akEKd2x/+Lsz2ze9CZX4/t4DzeFvU5vDwTCS9vfNVdUh1ZVVsl0bwqZCdInIccCbw5xpFW41Lr7yW9xZ+wPYd39K6U0/u/N2tvL/wA1au+gwRaPujNjzx0H21GULEirds5cprbyQQCFBeXs7FFwznp+eeTUlJCeN++St69P0JqampPP/kQ75r9qSkpPDI/fdwzoifEwiUM/aK0XTzUSIJdW6vuOZ6Vq5ajYjQtk1rnnjor16HehS/n9sg8X1nRiQk1PUk51aVhsC7BMfbqvgf2ASYrapdw+5Y5BTgBSCZYA1zmqqGvWktr1eOLl04z0383tFyryNwxe/PWR5JywPVr+QTsXRu8waeSf7ylTX6a5rXpokuubVfROsm3/D2MrcjOdeVcDW/a4GbgQxgGYeT3y7g0ep2rKqrgNwaxmeM8Z34mLQ83Hh+DwIPisgNqvpwHcZkjPGzBBrSqlxETqx4IyJNRcS/t8gbY2pfHHR4RBLdNU5vLQCq+h1wTa1FZIzxvzi41SWSUV2SREScmdARkWQgtXbDMsb4V3z09kaS/OYA00TkCYKPp11HBDMjGWPiVJxc84sk+d0GjCc4NZwAK4BWYbcwxsSx+OjtjeQJj3LgI+ArIA8YQnBOD2NMoorna34i0gkYBYwGdgD/BFDVwXUTmjHGt+Kg5heu2ftfYCHwM1VdDyAiNny9MQnP/7W6SIRL3xcC3wDvishTIjKEw095GGMSVbwPZqqq01X1EqALwRFZfgW0FJHHReTsOorPGOM7khg3OavqXlV9WVV/SnBcvpXApNoOzBjjY3HQ4eEqNavqt6r6pKqeUVsBGWN8LlHG8zPGmKP4vFYXCUt+xhiXEufxNmOM+SGfN2kj4bvk57ch20PRkv1eh+BO/eO9jsCdGPk9SEgx0JkRidhP38aYuhelDg8ReVZEtorI6kplzURknoh84fxsWumz34rIehFZJyLnVCrvLSKfOp89JBHUoiz5GWPcS5LIluo9z9GzOk4C5qtqR2C+8x4RySb4yG03Z5vHnCH2AB4nOABLR2epdso7S37GGPeidJ+fqi4Avj2ieATByc9wfo6sVD5VVQ+qagGwnuB84K2AJqq62Bl39MVK24Tku2t+xhifE1e9vc1FJL/S+ymqOqWabVqqajGAqhaLSAunPJPgCFMVCp2yUuf1keVhWfIzxrgXeW/v9ihOXVlVVVLDlIdlzV5jjEu1/mzvFqcpi/Nzq1NeCLSutF4WsNkpz6qiPCxLfsYY92o3+c0ExjivxwAzKpWPEpH6ItKOYMfGEqeJvFtE+ju9vFdU2iYka/YaY9yJ4hweIvIKcDrBa4OFwB3AvQTnDRoHfA1cDKCqn4nINGANUAZMVNWAs6sJBHuOjyM4x1C18wxZ8jPGuBS9OTxUdXSIj4aEWH8yMLmK8nygu5tjW/Izxrhnz/YaYxJPfMzeZsnPGONOxXh+Mc6SnzHGJav5GWMSVRyM6mLJzxjjkkBS7KeO2P8Gxpi6JUBS7Dd7Y/8bVCMQCJA7YDA/vfBSr0Nh7PW30rJTHj1OPeeoz+57eApJzdqxfUdwgIsly1aSO+g8cgedR85p5zJ91py6Djest+bOp3NOfzr06MO99z3odTg/sKmwiDPOPZ/sXgPpnjeIBx8NPke/ctVqBgw+l9wBZ9DntLNZkr/c40ir5udze0iizd4Wix58dApdO3fyOgwArrz0Qma/+vxR5ZsKN/P2e4tok5VxqKx7184sfWcmKxa8yexXX+C6W35HWVlZHUYbWiAQYOItk5g9fSprln3AK69OZ83adV6HdUhKSgr33XMXa5YvYvG7b/LYU8+xZu06bvv93fzvb3/DisXvcNfvb+W23//B61CP4vdzG5Qg8/bGssKizbzx1jyuvvJyr0MBYNCp/WjW9MSjym/53R/4812TfjCEf8OGx5GSErwqceDgQV/9EV2Sv5wO7dvSvl1bUlNTGXXRSGbMqvZpojrTKr0lvXJOAaBx4+Pp2rkjRcXfICLs2rUbgO+/30VGq5Zehlklv5/bQ+Ig+cX1Nb+bb/0df5l8B7t37/E6lJBmzp5HRqt0enbPPuqzj/NXMO6G29hYWMSLjz9wKBl6rWhzMa2zDg+XlpWZwcf5yzyMKLQNG79mxSer6ZfXi7/9+Q8MHTmK//ndXZSXl/PB/Fleh3eUmDi3UXy210v+Ts01MGv2XFqkpdE7t6fXoYS0b99+/nT/o9x9+6+q/LxfXi6rF89lydszuPfvj3HgwME6jrBqwcFyf8iPE0/t2bOXiy4bx9/+/AeaNGnM408/zwP33s3X61bwwL13c/Uvqz7vXoqNc+v09kay+JjnyU9ExotIvojkb9u+I2r7/WDxx8x84y3adu3FqDHX8M77i7h87ISo7T8avtywkYKvC8k57Tza9RxI4eZv6H36z/hmy7YfrNe1cwcaNWzIap9c+8nKzGBTYdGh94VFm8lIT/cwoqOVlpZy0WVjufSSC7lgxDAAXvzHtEOvL75gOEuWrfAyxCrFwrkF4qLZ63l0qjpFVfNUNS+t+UlR2+89d/8/Cr9YxYa1y5n6wlOc8ZOBvPTs41HbfzT0yO7Cls/zKfhkEQWfLCIrI51l771Oess0CjZuOtTBsXFTIevWf0XbNlnV7LFu9OmdyxdfFlCwYSMlJSVMfe0/DB9W7XwxdUZVufqXv6JL547ccsN1h8oz0tN5f+GHALzz3kI6ntzeqxBD8vu5DYqwp9d3NdYf8ne9NM5cevWNvPfBR2zf8R2tuw3gzkk3M+4Xl1S57qKPlvLnvz9BvXopJCUl8ehf/0Dzk5rVccRVS0lJ4ZH77+GcET8nEChn7BWj6ZbdxeuwDvlg8RL+75VX6dGtK7kDzgBg8p23M+WR+7n51t9TVlZGgwb1efLh+zyO9Gh+P7dA3DzbK1VdY/BKXq8czV/0ttdhREQP+rcTpSoSY5OWq5Z7HULEJIYSQd7AM8lfvrJGVbK8rm10yXO3RbRu8oDrl0VxDo+ospqfMcYlV7O3+ZYlP2OMezFU2w3Fkp8xxh2xIa2MMYkqyd89uZGw5GeMcc9qfsaYxGPNXmNMIhLr7TXGJCy75meMSUTW7DXGJCSfP7cbCUt+xhiXBB+MiVJjlvyMMe5Zzc8Yk3BEQKy31xiTiKzmZ4xJSJb8jDGJKfY7PGL/Gxhj6lj0hrEXkQ0i8qmIrBSRfKesmYjME5EvnJ9NK63/WxFZLyLrROScmnwLq/kdo1gbGTnWxNLoyAkpuh0eg1V1e6X3k4D5qnqviExy3t8mItnAKKAbkAG8LSKdVDVwLAe13zBjjDsV8/bW3gRGI4AXnNcvACMrlU9V1YOqWgCsB/oe60Es+RljXBI3U1c2r5ia1lnGH7EzBeaKyLJKn7VU1WIA52cLpzwT2FRp20Kn7JhYs9cY45qLidS3VzOB0Y9VdbOItADmich/wx22irJjnoHNan7GmGOQFOESnqpudn5uBaYTbMZuEZFWAM7Prc7qhUDrSptnAZtr8g2MMcaF6PT2ikgjEWlc8Ro4G1gNzATGOKuNAWY4r2cCo0Skvoi0AzoCS471W1iz1xjjXnR6e1sC050mdArwD1V9S0SWAtNEZBzwNXAxgKp+JiLTgDVAGTDxWHt6Kw5ojDGRq+jtrSFV/QroWUX5DmBIiG0mA5NrfHAs+RljXKvRbSy+YcnPGHMMYr+7wJKfMcY9q/kZYxKPTV1pjElUlvyMMQlHsORnjElE1ttrjElYlvyMMYnImr3GmMQTH729sf8Nwti583suuuwquuQOoGuvU1n88VKvQwrpwUefpHveaXTLG8jfH3nC63Cq9dbc+XTO6U+HHn24974HvQ4npHWfryen/+mHlibp7Xx/fmPj3EqEi3/Fdc3vpv+5naFnncFrLz9HSUkJ+/bt9zqkKq3+bC1PPfcSSxbMITU1laEjLmHY0LPo2OFkr0OrUiAQYOItk5j3+qtkZWbQ57SzGT5sKNldO3sd2lE6d+rAyo/eA4JxZ3bowfnDh3kbVBgxc27joMMjbmt+u3btZsEHHzFuzOUApKamcuKJJ3gcVdXWrvuc/n1707BhQ1JSUvjJaacyfeabXocV0pL85XRo35b27dqSmprKqItGMmPWbK/Dqtb8dxdwcvu2/KhN6+pX9kisnttYFLfJ76uCDaQ1P4mrrr2B3AGDufqXN7N3716vw6pS9+yuLPhgMTt2fMu+fft4c87bbCoq8jqskIo2F9M66/Do4VmZGRQVF3sYUWSmvjad0Rdf4HUYYcXEua39OTzqhOfJT0TGV4zvv237jqjttywQYPnKVUy45ipWLH6XRg0bcu/9D0Vt/9HUtUsnbrvlBs762UUMHXkJPXt0IyXZv1ckVI8eOdzFsOaeKCkpYeabc7j4/OFehxJW7Jzb2L/m53nyU9UpqpqnqnlpzU+K2n6zMlqRlZlBvz69Abjo/J+xfOWqqO0/2saNuZzlH77Dgrmv06zpiXTs0N7rkELKysxgU+Hhmmlh0WYy0tM9jKh6s+fOp1fPU2jZskX1K3soNs6tqwmMfMvf0dVAenpLWmdlsO7z9QDMf28h2V18dtG4kq1btwHw9aZC/j3zDV83z/r0zuWLLwso2LCRkpISpr72H4YPG+p1WGG98uq/GX3x+V6HUa2YObdx0Oz1b9sqCh6+7x4uG3sdJSWltG/3I557wp/NXoALL7uKHd9+R72Uejz6wJ9p2vREr0MKKSUlhUfuv4dzRvycQKCcsVeMplt2F6/DCmnfvn3Me+d9nnzofq9DqVbsnFt/J7ZISFXXGLyS1ytH8xe97XUYxsStvIFnkr98ZY0yV15Od106/98RrZvUvPOyaqau9Exc1/yMMbXB/50ZkbDkZ4xxz+fX8yJhyc8Ycwws+RljEo349d5Ddyz5GWOOgSU/Y0zC8f89fJGw5GeMOQaW/Iwxicjnj65FwpKfMcY9a/YaYxKP3eRsjElEFeP5xThLfsaYY2DJzxiTiGI/91nyM8Ycgzjo7Y39b2CMqWORDmFfffVQRIaKyDoRWS8ik2ot5CpY8jPGuBeFkZxFJBl4FDgXyAZGi0h2HUQPWPIzxhyTqNT8+gLrVfUrVS0BpgIjaiviI/nqmt+yFZ9sl0ZpG6O82+bA9ijvszbFUryxFCvEVry1FeuParqDZSs+mSON0ppHuHoDEcmv9H6Kqk5xXmcCmyp9Vgj0q2l8kfJV8lPVtGjvU0Ty/TqMdlViKd5YihViK14/x6qq0ZpRqaqqYZ3Nq2HNXmOMVwqB1pXeZwGb6+rglvyMMV5ZCnQUkXYikgqMAmbW1cF91eytJVOqX8VXYineWIoVYiveWIr1mKhqmYhcD8wBkoFnVfWzujq+r6auNLVDRALApwT/2K0FxqjqvmPc1/PALFV9TUSeBh5Q1TUh1j0dKFHVD10eYwOQp6qx0jlhYpA1exPDflXNUdXuQAlwXeUPnfutXFPVq0MlPsfpwKnHsm9japslv8SzEOggIqeLyLsi8g/gUxFJFpG/ishSEVklItcCSNAjIrJGRN4AWlTsSETeE5E85/VQEVkuIp+IyHwRaUswyf5KRFaKyGkikiYi/3KOsVREfuxse5KIzBWRFSLyJHHx5Kjxu0S45mccIpJC8G76t5yivkB3VS0QkfHA96raR0TqAx+IyFwgF+gM9ABaAmuAZ4/YbxrwFDDI2VczVf1WRJ4A9qjqfc56/wD+pqqLRKQNwWs9XYE7gEWqereIDAPG1+qJMAZLfoniOBFZ6bxeCDxDsDm6RFULnPKzgVNE5CLn/QlAR2AQ8IqqBoDNIvJOFfvvDyyo2JeqfhsijjOB7ErTHjYRkcbOMS5wtn1DRL47tq9pTOQs+SWG/aqaU7nASUB7KxcBN6jqnCPWO4/qbzyVCNaB4GWWAaq6v4pYrOfN1Cm75mcqzAEmiEg9ABHpJCKNgAXAKOeaYCtgcBXbLgZ+IiLtnG2bOeW7gcaV1psLXF/xRkRynJcLgMucsnOBptH6UsaEYsnPVHia4PW85SKyGniSYMtgOvAFwVtlHgfeP3JDVd1G8Drdv0XkE+CfzkevA+dXdHgANwJ5TofKGg73Ot8FDBKR5QSb31/X0nc05hC7z88Yk5Cs5meMSUiW/IwxCcmSnzEmIVnyM8YkJEt+xpiEZMnPGJOQLPkZYxLS/wdwllin2xnklwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 使用matplotlib绘制混淆矩阵\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(cm, cmap=plt.cm.Oranges)  # 选择颜色映射\n",
    "\n",
    "# 为图添加颜色条\n",
    "plt.colorbar(cax)\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_xticklabels([''] + list(np.unique(y_pred)))\n",
    "ax.set_yticklabels([''] + list(np.unique(y_pred)))\n",
    "\n",
    "# 在混淆矩阵的各个单元格中添加数值标签\n",
    "for (i, j), val in np.ndenumerate(cm):\n",
    "    ax.text(j, i, f'{val}', ha='center', va='center', color='black')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy实现LSTM前向推理过程\n",
    "\n",
    "基于numpy实现推理的流程，很容易实现基于C代码实现LSTM的前向推理。\n",
    "\n",
    "### LSTM推理公式\n",
    "\n",
    "\\begin{array}{ll} \\\\\n",
    "    i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "    f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "    g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "    o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "    c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "    h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于numpy实现的LSTM前向推理\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_parameters(model):\n",
    "    parameters = {}\n",
    "    # LSTM层的权重和偏置\n",
    "    parameters['lstm_weights_ih'] = [p.data.numpy() for p in model.lstm.parameters()][0::4]\n",
    "    parameters['lstm_weights_hh'] = [p.data.numpy() for p in model.lstm.parameters()][1::4]\n",
    "    parameters['lstm_biases_ih'] = [p.data.numpy() for p in model.lstm.parameters()][2::4]\n",
    "    parameters['lstm_biases_hh'] = [p.data.numpy() for p in model.lstm.parameters()][3::4]\n",
    "    \n",
    "    # 全连接层的权重和偏置\n",
    "    parameters['fc_weights'] = model.fc.weight.data.numpy()\n",
    "    parameters['fc_biases'] = model.fc.bias.data.numpy()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def lstm_forward(x, params, hidden_dim, num_layers):\n",
    "    seq_len, input_dim = x.shape\n",
    "    h = np.zeros((num_layers, hidden_dim))\n",
    "    c = np.zeros((num_layers, hidden_dim))\n",
    "    \n",
    "    # for t in range(seq_len):\n",
    "    for layer in range(num_layers):        \n",
    "        print(f'执行第{layer}层')\n",
    "        if layer == 0:\n",
    "            input, hx = x, np.zeros((1, hidden_dim))\n",
    "        else:\n",
    "            input, hx = h[layer-1], np.zeros((1, hidden_dim))\n",
    "        \n",
    "        wi, wf, wg, wo = np.split(params['lstm_weights_ih'][layer], 4)\n",
    "        bi, bf, bg, bo = np.split(params['lstm_biases_ih'][layer], 4)\n",
    "        whi, whf, whg, who = np.split(params['lstm_weights_hh'][layer], 4)\n",
    "        bhi, bhf, bhg, bho = np.split(params['lstm_biases_hh'][layer], 4)\n",
    "        \n",
    "        # print(params['lstm_weights_ih'][layer].shape)\n",
    "        # print(params['lstm_biases_ih'][layer].shape)\n",
    "        # print(params['lstm_weights_hh'][layer].shape)\n",
    "        # print(params['lstm_biases_hh'][layer].shape)\n",
    "        # print(wi.shape, bi.shape, whi.shape, bhi.shape)\n",
    "\n",
    "        # i = sigmoid(np.dot(input, wi.T) + bi + np.dot(hx, whi.T) + bhi)\n",
    "        # f = sigmoid(np.dot(input, wf.T) + bf + np.dot(hx, whf.T) + bhf)\n",
    "        # g = tanh(np.dot(input, wg.T) + bg + np.dot(hx, whg.T) + bhg)\n",
    "        # o = sigmoid(np.dot(input, wo.T) + bo + np.dot(hx, who.T) + bho)\n",
    "        \n",
    "        # 进一步简化\n",
    "        i = sigmoid(np.dot(input, wi.T) + bi + bhi)\n",
    "        f = sigmoid(np.dot(input, wf.T) + bf + bhf)\n",
    "        g = tanh(np.dot(input, wg.T) + bg + bhg)\n",
    "        o = sigmoid(np.dot(input, wo.T) + bo + bho)\n",
    "        \n",
    "        c[layer] = f * c[layer] + i * g\n",
    "        h[layer] = o * tanh(c[layer])\n",
    "    \n",
    "    return h[-1]  # Returning the last hidden state of the last layer\n",
    "\n",
    "def fc_forward(h, params):\n",
    "    return np.dot(params['fc_weights'], h) + params['fc_biases']\n",
    "\n",
    "def model_forward(x, params, num_layers):\n",
    "    h = lstm_forward(x, params, params['fc_weights'].shape[1], num_layers)\n",
    "    output = fc_forward(h, params)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0: torch.Size([16, 140])\n",
      "lstm.weight_hh_l0: torch.Size([16, 4])\n",
      "lstm.bias_ih_l0: torch.Size([16])\n",
      "lstm.bias_hh_l0: torch.Size([16])\n",
      "lstm.weight_ih_l1: torch.Size([16, 4])\n",
      "lstm.weight_hh_l1: torch.Size([16, 4])\n",
      "lstm.bias_ih_l1: torch.Size([16])\n",
      "lstm.bias_hh_l1: torch.Size([16])\n",
      "fc.weight: torch.Size([5, 4])\n",
      "fc.bias: torch.Size([5])\n",
      "--------------------------------------------\n",
      "执行第0层\n",
      "执行第1层\n",
      "[ 5.82873522 -3.73747235 -0.81065949 -1.4431227  -0.92429621] \n",
      " [[ 5.8287354  -3.7374723  -0.8106595  -1.4431226  -0.92429626]]\n",
      "预测类型：0, 真值： 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 打印模型的权重\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "print('--------------------------------------------')\n",
    "\n",
    "# 将模型权重转换为 NumPy 数组\n",
    "params = {name: param.detach().cpu().numpy() for name, param in model.named_parameters()}\n",
    "\n",
    "params = extract_parameters(model)\n",
    "\n",
    "# 读取测试数据集\n",
    "data_test = np.loadtxt('./data/ECG5000_TEST.txt')\n",
    "\n",
    "# 获取data和label\n",
    "test_data, test_labels = data_test[:, 1:], data_test[:, 0]-1.0\n",
    "\n",
    "# 随机选择一个测试样本\n",
    "rd_idx = np.random.randint(test_labels.shape[0])\n",
    "rd_idx = 100\n",
    "input_data = test_data[rd_idx,:]\n",
    "\n",
    "# def print_cpp_format(array, array_name=\"arr\"):\n",
    "#     # 将数组元素转换为字符串列表\n",
    "#     elements = map(str, array)\n",
    "#     # 拼接所有元素为一个逗号分隔的字符串\n",
    "#     elements_str = ', '.join(elements)\n",
    "#     # 打印C++风格的数组初始化代码\n",
    "#     print(f\"double {array_name}[] = {{{elements_str}}};\")\n",
    "\n",
    "# print_cpp_format(input_data)\n",
    "\n",
    "input_data = input_data.reshape([1,140])\n",
    "\n",
    "# 执行 LSTM 前向传播\n",
    "output = model_forward(input_data, params, model.lstm.num_layers)\n",
    "\n",
    "# 使用原本model进行推理\n",
    "model.eval()\n",
    "input_data = torch.tensor(input_data.reshape([1,1,140]), dtype=torch.float32).to(device=device)\n",
    "output2 = model(input_data)\n",
    "output2 = output2.detach().cpu().numpy()\n",
    "\n",
    "# 打印两种推理方式的结果，可以看到是一致的\n",
    "print(output, '\\n' , output2)\n",
    "\n",
    "# 预测label\n",
    "predict_ = np.argmax(output)\n",
    "\n",
    "print(f\"预测类型：{predict_}, 真值： {test_labels[rd_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行第0层\n",
      "执行第1层\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_lstm_parameters(weights, filename):\n",
    "    # 分割权重和偏置\n",
    "    wi, wf, wg, wo = np.split(weights['lstm.weight_ih_l0'], 4)\n",
    "    bi, bf, bg, bo = np.split(weights['lstm.bias_ih_l0'], 4)\n",
    "    whi, whf, whg, who = np.split(weights['lstm.weight_hh_l0'], 4)\n",
    "    bhi, bhf, bhg, bho = np.split(weights['lstm.bias_hh_l0'], 4)\n",
    "    \n",
    "    # 打开文件进行写入\n",
    "    with open(filename, 'w') as file:\n",
    "        # 写入输入门权重和偏置\n",
    "        file.write(\"Input Gate Weights (wi):\\n\")\n",
    "        file.write(np.array2string(wi) + \"\\n\")\n",
    "        file.write(\"Input Gate Bias (bi):\\n\")\n",
    "        file.write(np.array2string(bi) + \"\\n\")\n",
    "        \n",
    "        # 写入遗忘门权重和偏置\n",
    "        file.write(\"Forget Gate Weights (wf):\\n\")\n",
    "        file.write(np.array2string(wf) + \"\\n\")\n",
    "        file.write(\"Forget Gate Bias (bf):\\n\")\n",
    "        file.write(np.array2string(bf) + \"\\n\")\n",
    "        \n",
    "        # 写入候选门权重和偏置\n",
    "        file.write(\"Candidate Gate Weights (wg):\\n\")\n",
    "        file.write(np.array2string(wg) + \"\\n\")\n",
    "        file.write(\"Candidate Gate Bias (bg):\\n\")\n",
    "        file.write(np.array2string(bg) + \"\\n\")\n",
    "        \n",
    "        # 写入输出门权重和偏置\n",
    "        file.write(\"Output Gate Weights (wo):\\n\")\n",
    "        file.write(np.array2string(wo) + \"\\n\")\n",
    "        file.write(\"Output Gate Bias (bo):\\n\")\n",
    "        file.write(np.array2string(bo) + \"\\n\")\n",
    "        \n",
    "        # 写入隐藏状态权重和偏置\n",
    "        file.write(\"Hidden State Weights (whi, whf, whg, who):\\n\")\n",
    "        file.write(np.array2string(whi) + \"\\n\")\n",
    "        file.write(np.array2string(whf) + \"\\n\")\n",
    "        file.write(np.array2string(whg) + \"\\n\")\n",
    "        file.write(np.array2string(who) + \"\\n\")\n",
    "        \n",
    "        file.write(\"Hidden State Biases (bhi, bhf, bhg, bho):\\n\")\n",
    "        file.write(np.array2string(bhi) + \"\\n\")\n",
    "        file.write(np.array2string(bhf) + \"\\n\")\n",
    "        file.write(np.array2string(bhg) + \"\\n\")\n",
    "        file.write(np.array2string(bho) + \"\\n\")\n",
    "\n",
    "\n",
    "def extract_parameters(model):\n",
    "    parameters = {}\n",
    "    # LSTM层的权重和偏置\n",
    "    parameters['lstm_weights_ih'] = [p.data.numpy() for p in model.lstm.parameters()][0::4]\n",
    "    parameters['lstm_weights_hh'] = [p.data.numpy() for p in model.lstm.parameters()][1::4]\n",
    "    parameters['lstm_biases_ih'] = [p.data.numpy() for p in model.lstm.parameters()][2::4]\n",
    "    parameters['lstm_biases_hh'] = [p.data.numpy() for p in model.lstm.parameters()][3::4]\n",
    "    \n",
    "    # 全连接层的权重和偏置\n",
    "    parameters['fc_weights'] = model.fc.weight.data.numpy()\n",
    "    parameters['fc_biases'] = model.fc.bias.data.numpy()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def save_lstm_parameters_as_cpp(model, filename):\n",
    "    \n",
    "    # 将模型权重转换为 NumPy 数组\n",
    "    params = extract_parameters(model)\n",
    "    num_layers = model.lstm.num_layers\n",
    "\n",
    "    # 打开文件进行写入\n",
    "    with open(filename, 'w') as file:\n",
    "        def write_array(name, array):\n",
    "            file.write(f\"double {name}[] = {{{', '.join(map(str, array.flatten()))}}};\\n\")\n",
    "        \n",
    "        # 分割权重和偏置\n",
    "        for layer in range(num_layers):        \n",
    "            print(f'执行第{layer}层')\n",
    "            wi, wf, wg, wo = np.split(params['lstm_weights_ih'][layer], 4)\n",
    "            bi, bf, bg, bo = np.split(params['lstm_biases_ih'][layer], 4)\n",
    "            whi, whf, whg, who = np.split(params['lstm_weights_hh'][layer], 4)\n",
    "            bhi, bhf, bhg, bho = np.split(params['lstm_biases_hh'][layer], 4)\n",
    "            \n",
    "            # 写入输入门权重和偏置\n",
    "            write_array(\"wi\" + str(layer), wi)\n",
    "            write_array(\"bi\" + str(layer), bi)\n",
    "            \n",
    "            # 写入遗忘门权重和偏置\n",
    "            write_array(\"wf\" + str(layer), wf)\n",
    "            write_array(\"bf\" + str(layer), bf)\n",
    "            \n",
    "            # 写入候选门权重和偏置\n",
    "            write_array(\"wg\" + str(layer), wg)\n",
    "            write_array(\"bg\" + str(layer), bg)\n",
    "            \n",
    "            # 写入输出门权重和偏置\n",
    "            write_array(\"wo\" + str(layer), wo)\n",
    "            write_array(\"bo\" + str(layer), bo)\n",
    "            \n",
    "            # 写入隐藏状态权重和偏置\n",
    "            write_array(\"whi\" + str(layer), whi)\n",
    "            write_array(\"whf\" + str(layer), whf)\n",
    "            write_array(\"whg\" + str(layer), whg)\n",
    "            write_array(\"who\" + str(layer), who)\n",
    "            \n",
    "            write_array(\"bhi\" + str(layer), bhi)\n",
    "            write_array(\"bhf\" + str(layer), bhf)\n",
    "            write_array(\"bhg\" + str(layer), bhg)\n",
    "            write_array(\"bho\" + str(layer), bho)\n",
    "        \n",
    "        fc_w, fc_b = params['fc_weights'], params['fc_biases']\n",
    "        write_array(\"fc_w\", fc_w) # 注意此处的转置T\n",
    "        write_array(\"fc_b\", fc_b)\n",
    "        \n",
    "\n",
    "# 保存参数文件\n",
    "# save_lstm_parameters(params, 'lstm_parameters.txt')\n",
    "\n",
    "save_lstm_parameters_as_cpp(model, 'lstm_parameters.cpp')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
