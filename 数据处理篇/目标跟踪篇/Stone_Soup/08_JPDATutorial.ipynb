{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 8 - Joint probabilistic data association tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we have multiple targets we're going to want to arrive at a globally-consistent collection\n",
        "of associations for PDA, in much the same way as we did for the global nearest neighbour\n",
        "associator. This is the purpose of the *joint* probabilistic data association (JPDA) filter.\n",
        "\n",
        "Similar to the PDA, the JPDA algorithm calculates hypothesis pairs for every measurement\n",
        "for every track. The probability of a track-measurement hypothesis is calculated by the sum of\n",
        "normalised conditional probabilities that every other track is associated to every other\n",
        "measurement (including missed detection). For example, with 3 tracks $(A, B, C)$ and 3\n",
        "measurements $(x, y, z)$ (including missed detection $None$), the probability of\n",
        "track $A$ being associated with measurement $x$ ($A \\to x$) is given by:\n",
        "\n",
        "\\begin{align}p(A \\to x) &= \\bar{p}(A \\to x \\cap B \\to None \\cap C \\to None) +\\\\\n",
        "                 &+ \\bar{p}(A \\to x \\cap B \\to None \\cap C \\to y) +\\\\\n",
        "                 &+ \\bar{p}(A \\to x \\cap B \\to None \\cap C \\to z) +\\\\\n",
        "                 &+ \\bar{p}(A \\to x \\cap B \\to y \\cap C \\to None) +\\\\\n",
        "                 &+ \\bar{p}(A \\to x \\cap B \\to y \\cap C \\to z) +\\\\\n",
        "                 &+ \\bar{p}(A \\to x \\cap B \\to z \\cap C \\to None) +\\\\\n",
        "                 &+ \\bar{p}(A \\to x \\cap B \\to z \\cap C \\to y)\\end{align}\n",
        "\n",
        "where $\\bar{p}(\\textit{multi-hypothesis})$ is the normalised probability of the\n",
        "multi-hypothesis.\n",
        "\n",
        "This is demonstrated for 2 tracks associating to 3 measurements in the diagrams below:\n",
        "\n",
        "<img src=\"https://stonesoup.rtfd.io/en/v1.2/_static/jpda_diag_1.png\" width=\"250\" height=\"300\" alt=\"Image showing two tracks approaching 3 detections with associated probabilities\">\n",
        "\n",
        "Where the probability (for example) of the orange track associating to the green measurement is\n",
        "$0.25$.\n",
        "The probability of every possible association set is calculated. These probabilities are then\n",
        "normalised.\n",
        "\n",
        "<img src=\"https://stonesoup.rtfd.io/en/v1.2/_static/jpda_diag_2.png\" width=\"350\" height=\"300\" alt=\"Image showing calculation of the conditional probabilities of every possible occurrence\">\n",
        "\n",
        "A track-measurement hypothesis weight is then recalculated as the sum of the probabilities of\n",
        "every occurrence where that track associates to that measurement.\n",
        "\n",
        "<img src=\"https://stonesoup.rtfd.io/en/v1.2/_static/jpda_diag_3.png\" width=\"500\" height=\"450\" alt=\"Image showing the recalculated probabilities of each track-measurement hypothesis\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate ground truth\n",
        "As with the multi-target data association tutorial, we simulate two targets moving in the\n",
        "positive x, y Cartesian plane (intersecting approximately half-way through their transition).\n",
        "We then add truth detections with clutter at each time-step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from ordered_set import OrderedSet\n",
        "import numpy as np\n",
        "from scipy.stats import uniform\n",
        "\n",
        "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
        "                                               ConstantVelocity\n",
        "from stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState\n",
        "from stonesoup.types.detection import TrueDetection\n",
        "from stonesoup.types.detection import Clutter\n",
        "from stonesoup.models.measurement.linear import LinearGaussian\n",
        "\n",
        "np.random.seed(1991)\n",
        "\n",
        "truths = OrderedSet()\n",
        "\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.005),\n",
        "                                                          ConstantVelocity(0.005)])\n",
        "\n",
        "timesteps = [start_time]\n",
        "truth = GroundTruthPath([GroundTruthState([0, 1, 0, 1], timestamp=timesteps[0])])\n",
        "for k in range(1, 21):\n",
        "    timesteps.append(start_time + timedelta(seconds=k))\n",
        "    truth.append(GroundTruthState(\n",
        "        transition_model.function(truth[k-1], noise=True, time_interval=timedelta(seconds=1)),\n",
        "        timestamp=timesteps[k]))\n",
        "truths.add(truth)\n",
        "\n",
        "truth = GroundTruthPath([GroundTruthState([0, 1, 20, -1], timestamp=timesteps[0])])\n",
        "for k in range(1, 21):\n",
        "    truth.append(GroundTruthState(\n",
        "        transition_model.function(truth[k-1], noise=True, time_interval=timedelta(seconds=1)),\n",
        "        timestamp=timesteps[k]))\n",
        "truths.add(truth)\n",
        "\n",
        "# Plot ground truth.\n",
        "from stonesoup.plotter import AnimatedPlotterly\n",
        "plotter = AnimatedPlotterly(timesteps, tail_length=0.3)\n",
        "plotter.plot_ground_truths(truths, [0, 2])\n",
        "\n",
        "# Generate measurements.\n",
        "all_measurements = []\n",
        "\n",
        "measurement_model = LinearGaussian(\n",
        "    ndim_state=4,\n",
        "    mapping=(0, 2),\n",
        "    noise_covar=np.array([[0.75, 0],\n",
        "                          [0, 0.75]])\n",
        "    )\n",
        "\n",
        "prob_detect = 0.9  # 90% chance of detection.\n",
        "\n",
        "for k in range(20):\n",
        "    measurement_set = set()\n",
        "\n",
        "    for truth in truths:\n",
        "        # Generate actual detection from the state with a 10% chance that no detection is received.\n",
        "        if np.random.rand() <= prob_detect:\n",
        "            measurement = measurement_model.function(truth[k], noise=True)\n",
        "            measurement_set.add(TrueDetection(state_vector=measurement,\n",
        "                                              groundtruth_path=truth,\n",
        "                                              timestamp=truth[k].timestamp,\n",
        "                                              measurement_model=measurement_model))\n",
        "\n",
        "        # Generate clutter at this time-step\n",
        "        truth_x = truth[k].state_vector[0]\n",
        "        truth_y = truth[k].state_vector[2]\n",
        "        for _ in range(np.random.randint(10)):\n",
        "            x = uniform.rvs(truth_x - 10, 20)\n",
        "            y = uniform.rvs(truth_y - 10, 20)\n",
        "            measurement_set.add(Clutter(np.array([[x], [y]]), timestamp=truth[k].timestamp,\n",
        "                                        measurement_model=measurement_model))\n",
        "    all_measurements.append(measurement_set)\n",
        "\n",
        "# Plot true detections and clutter.\n",
        "plotter.plot_measurements(all_measurements, [0, 2])\n",
        "plotter.fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.predictor.kalman import KalmanPredictor\n",
        "predictor = KalmanPredictor(transition_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.updater.kalman import KalmanUpdater\n",
        "updater = KalmanUpdater(measurement_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initial hypotheses are calculated (per track) in the same manner as the PDA.\n",
        "Therefore, in Stone Soup, the JPDA filter uses the :class:`~.PDAHypothesiser` to create these\n",
        "hypotheses.\n",
        "Unlike the :class:`~.PDA` data associator, in Stone Soup, the :class:`~.JPDA` associator takes\n",
        "this collection of hypotheses and adjusts their weights according to the method described above,\n",
        "before returning key-value pairs of tracks and detections to be associated with them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.hypothesiser.probability import PDAHypothesiser\n",
        "# This doesn't need to be created again, but for the sake of visualising the process, it has been\n",
        "# added.\n",
        "hypothesiser = PDAHypothesiser(predictor=predictor,\n",
        "                               updater=updater,\n",
        "                               clutter_spatial_density=0.125,\n",
        "                               prob_detect=prob_detect)\n",
        "\n",
        "from stonesoup.dataassociator.probability import JPDA\n",
        "data_associator = JPDA(hypothesiser=hypothesiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the JPDA filter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from stonesoup.types.state import GaussianState\n",
        "from stonesoup.types.track import Track\n",
        "from stonesoup.types.array import StateVectors\n",
        "from stonesoup.functions import gm_reduce_single\n",
        "from stonesoup.types.update import GaussianStateUpdate\n",
        "\n",
        "prior1 = GaussianState([[0], [1], [0], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
        "prior2 = GaussianState([[0], [1], [20], [-1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
        "\n",
        "tracks = {Track([prior1]), Track([prior2])}\n",
        "\n",
        "for n, measurements in enumerate(all_measurements):\n",
        "    hypotheses = data_associator.associate(tracks,\n",
        "                                           measurements,\n",
        "                                           start_time + timedelta(seconds=n))\n",
        "\n",
        "    # Loop through each track, performing the association step with weights adjusted according to\n",
        "    # JPDA.\n",
        "    for track in tracks:\n",
        "        track_hypotheses = hypotheses[track]\n",
        "\n",
        "        posterior_states = []\n",
        "        posterior_state_weights = []\n",
        "        for hypothesis in track_hypotheses:\n",
        "            if not hypothesis:\n",
        "                posterior_states.append(hypothesis.prediction)\n",
        "            else:\n",
        "                posterior_state = updater.update(hypothesis)\n",
        "                posterior_states.append(posterior_state)\n",
        "            posterior_state_weights.append(hypothesis.probability)\n",
        "\n",
        "        means = StateVectors([state.state_vector for state in posterior_states])\n",
        "        covars = np.stack([state.covar for state in posterior_states], axis=2)\n",
        "        weights = np.asarray(posterior_state_weights)\n",
        "\n",
        "        # Reduce mixture of states to one posterior estimate Gaussian.\n",
        "        post_mean, post_covar = gm_reduce_single(means, covars, weights)\n",
        "\n",
        "        # Add a Gaussian state approximation to the track.\n",
        "        track.append(GaussianStateUpdate(\n",
        "            post_mean, post_covar,\n",
        "            track_hypotheses,\n",
        "            track_hypotheses[0].measurement.timestamp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the resulting tracks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotter.plot_tracks(tracks, [0, 2], uncertainty=True)\n",
        "plotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "1. Bar-Shalom Y, Daum F, Huang F 2009, The Probabilistic Data Association Filter, IEEE Control\n",
        "Systems Magazine\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
